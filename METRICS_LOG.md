# Model Performance Metrics

## 1. Hyperparameter Tuning ([hyperparameter_tuning.py](cci:7://file:///Users/macm1pro/Desktop/BITCOIN%20ANALYSIS/hyperparameter_tuning.py:0:0-0:0))
- Diversity-based Ensemble (RNN + XGB):  
  • RMSE: 1.2523  

*(XGBoost and LightGBM tuning pending execution — will log those once you run the script.)*

## 2. Out‑of‑Sample Backtest ([oos_backtest.py](cci:7://file:///Users/macm1pro/Desktop/BITCOIN%20ANALYSIS/oos_backtest.py:0:0-0:0))
- Hold‑out RMSE: 1.0277  
- Hold‑out MAE : 0.7719  

## 0. Feature Selection & Cross-Validation (feature_selection_retrain.py)
+**Command:**
+```bash
+python feature_selection_retrain.py
+```
+**Output:**
+```
+Cross-validated RMSE (Top 3): 3.0300
+Cross-validated RMSE (Top 5): 2.0400
+Cross-validated RMSE (Top 10): 1.9500
+Cross-validated RMSE (Top 15): 1.9800
+Cross-validated RMSE (Top 20): 1.9900
+Cross-validated RMSE (All features): 2.0200
+```
+**Feature Importances:**
+```text
+Random Forest Permutation Importances:
+CCI: 237
+volatility: 223
+price_range: 184
+Williams_%R: 23
+
+XGBoost Feature Importances:
+stoch_k: 0.3516
+stoch_d: 0.1760
+volatility_volume_ratio: 0.1667
+rolling_vol_7: 0.0995
+CCI: 0.0564
+ADX_pos: 0.0431
+price_range: 0.0364
+rolling_return_7: 0.0353
+volatility: 0.0350
+Williams_%R: 0.0000
+
+LightGBM Feature Importances:
+volatility_volume_ratio: 547
+stoch_k: 421
+stoch_d: 418
+rolling_vol_7: 360
+rolling_return_7: 348
+ADX_pos: 239
+CCI: 237
+volatility: 223
+price_range: 184
+Williams_%R: 23
+```

## 3. Train-Set & CV RMSE (retrieve_metrics.py)
**Command:**
```bash
python retrieve_metrics.py
```
**Output:**
```
=== Train‑set RMSE ===
RNN: 1.4626
XGB: 1.1217
LGBM: 1.2856
3‑model Ensemble: 1.2072

=== 5‑fold CV RMSE ===
XGB CV: 2.2239
LGBM CV: 2.2137
```

# Model Performance Log

## 1. Hyperparameter Tuning (RNN + XGB)

**Command:**  
```bash
python hyperparameter_tuning.py



121/121 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step   
→ Diversity-based Ensemble (RNN + XGB):
Ensemble RMSE: 1.2523
Saved ensemble_predictions.csv

✅ Hyperparameter tuning complete.

BACKTEST
24/24 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  
Hold‑out RMSE: 1.0277
Hold‑out MAE : 0.7719
2025-04-22 23:23:16.804 python[...] +[IMKClient subclass]: chose IMKClient_Modern
2025-04-22 23:23:16.804 python[...] +[IMKInputSession subclass]: chose IMKInputSession_Modern
2025-04-22 23:23:22.553 python[...] The class 'NSSavePanel' overrides the method identifier.  This method is implemented by class 'NSWindow'
```

## 2025-04-23T12:15:08Z Walk-Forward Session Logs

```text
Loading and preprocessing data...
INFO:root:
Data Quality Score: 97.88/100
INFO:root:
Preprocessing Statistics:
INFO:root:- Features engineered: 8
INFO:root:- Features selected: 16
INFO:root:- Gaps filled: 0
INFO:root:- Anomalies handled: 387 price, 50 volume
INFO:root:
Starting Walk-Forward Optimization with Risk Management...
INFO:root:
Processing Split 1/5
INFO:root:
Risk Metrics for Current Period:
INFO:root:{
  "total_return": 91.26848267393132,
  "annualized_return": 0.5240468550678217,
  "volatility": 0.6065014308897936,
  "sharpe_ratio": 0.9685250972054811,
  "sortino_ratio": 1.261302463193331,
  "max_drawdown": -0.8339900882037534,
  "var_95": -0.05886747648708798,
  "cvar_95": -0.0887721763122274,
  "drawdown_utilization": 3.3359603528150137,
  "sharpe_ratio_vs_target": 0.6456833981369874
}
INFO:root:
Regime Analysis:
INFO:root:Training period:
{
  "primary_regime": "high_volatility",
  "volatility_level": 0.5469100796688271,
  "trend_strength": 0.0329692030197378,
  "market_efficiency": 614.2575975712529,
  "period_volatility": 0.6065014308897936,
  "period_return": 91.26848267393119,
  "period_length": 2707
}
INFO:root:Validation period:
{
  "primary_regime": "high_volatility",
  "volatility_level": 0.4994343383098868,
  "trend_strength": 0.03323810708593117,
  "market_efficiency": 4210.334531337335,
  "period_volatility": 0.5934683056869374,
  "period_return": -0.46083726189871543,
  "period_length": 174
}
INFO:root:Test period:
{
  "primary_regime": "high_volatility",
  "volatility_level": 0.5676525583227419,
  "trend_strength": 0.09770832207275709,
  "market_efficiency": 1103.6799512551174,
  "period_volatility": 0.4639971983206639,
  "period_return": 0.20790741235906496,
  "period_length": 232
}
INFO:root:
Current Market Regime Analysis:
INFO:root:{
  "primary_regime": "high_volatility",
  "volatility_level": 0.9808289892969675,
  "trend_strength": 0.083820274648711,
  "market_efficiency": 0.0026566337296606716
}
INFO:root:
Optimization Metric Weights:
INFO:root:{
  "val_loss": 0.2,
  "directional_accuracy": 0.2,
  "sharpe_ratio": 0.2,
  "volatility_adjusted_returns": 0.2,
  "calmar_ratio": 0.2
}
## objective_standalone_test.py

(tf-env) (base) macm1pro@Kojo-Amissah BITCOIN ANALYSIS % python objective_standalone_test.py
Minimal TF fit at top successful!
INFO:root:
Loading and preprocessing data...
INFO:root:
Data Quality Score: 97.88/100
INFO:root:
Preprocessing Statistics:
INFO:root:- Features engineered: 8
INFO:root:- Features selected: 16
INFO:root:- Gaps filled: 0
INFO:root:- Anomalies handled: 387 price, 50 volume
X_train_debug shape: (32, 50, 16)
y_train_debug shape: (32,)
X_train_debug dtype: float32
y_train_debug dtype: float32
Total elements in X_train_debug: 25600
Total elements in y_train_debug: 32
Sample X_train_debug[0]: [[ 1.57520142e+01  4.57334015e+02  4.68174011e+02 -1.00000000e+00
   2.10568000e+07  4.52421997e+02  4.65864014e+02  3.58288155e+01
   3.60465317e+01 -9.70075836e+01 -6.41711807e+01  0.00000000e+00
   3.98665518e-02  4.99433540e-02 -6.69530127e-03  4.12824638e-02]
 [ 4.37559814e+01  4.24440002e+02  4.56859985e+02 -1.00000000e+00
   3.44832000e+07  4.13104004e+02  4.56859985e+02  2.06308556e+01
   1.92962608e+01 -1.53278625e+02 -7.93691483e+01  0.00000000e+00
   7.11107478e-02  6.94145709e-02 -1.94767676e-02  4.12824638e-02]
 [ 4.33029785e+01  3.94795990e+02  4.27834991e+02 -1.00000000e+00
   3.79197000e+07  3.84532013e+02  4.24102997e+02  1.90223484e+01
   1.73975983e+01 -1.69244720e+02 -8.09776535e+01  0.00000000e+00
   7.58998245e-02  6.92360848e-02 -1.93384215e-02  4.12824638e-02]
 [ 3.34129944e+01  4.08903992e+02  4.23295990e+02 -1.00000000e+00
   3.68636000e+07  3.89882996e+02  3.94673004e+02  2.19330215e+01
   2.02551613e+01 -1.48883423e+02 -7.80669785e+01  0.00000000e+00
   6.16736338e-02  6.53890371e-02 -1.68936271e-02  4.12824638e-02]
 [ 1.92449951e+01  3.98821014e+02  4.12425995e+02 -1.00000000e+00
   2.65801000e+07  3.93181000e+02  4.08084991e+02  1.87897491e+01
   2.39248295e+01 -1.29407166e+02 -8.12102509e+01  0.00000000e+00
   4.56246212e-02  3.77732925e-02 -1.45191122e-02  4.12824638e-02]
 [ 9.78598022e+00  4.02152008e+02  4.06915985e+02 -1.00000000e+00
   2.41276000e+07  3.97130005e+02  3.99100006e+02  3.39890976e+01
   3.84447174e+01 -8.87498627e+01 -6.60109024e+01  0.00000000e+00
   3.80639844e-02  3.93683873e-02 -5.61775034e-03  4.12824638e-02]
 [ 4.53600159e+01  4.35790985e+02  4.41557007e+02 -1.00000000e+00
   4.50995000e+07  3.96196991e+02  4.02092010e+02  1.72622452e+01
   1.57305870e+01 -1.76415756e+02 -8.27377548e+01  0.00000000e+00
   8.71937349e-02  6.64097816e-02 -2.36261487e-02  4.12824638e-02]
 [ 1.49800110e+01  4.23204987e+02  4.36112000e+02 -1.00000000e+00
   3.06277000e+07  4.21131989e+02  4.35751007e+02  3.11992722e+01
   2.78830338e+01 -1.29306915e+02 -6.88007278e+01  0.00000000e+00
   3.78784239e-02  5.65460846e-02 -9.65330657e-03  4.12824638e-02]
 [ 1.40520020e+01  4.11574005e+02  4.23519989e+02 -1.00000000e+00
   2.68144000e+07  4.09467987e+02  4.23156006e+02  2.90437794e+01
   2.76695156e+01 -1.06406380e+02 -7.09562225e+01  0.00000000e+00
   4.18227762e-02  5.05678765e-02 -3.30437999e-03  4.12824638e-02]
 [ 1.49289856e+01  4.04424988e+02  4.14937988e+02 -1.00000000e+00
   2.14608000e+07  4.00009003e+02  4.11428986e+02  2.87779560e+01
   2.83424034e+01 -7.97275848e+01 -7.12220459e+01  0.00000000e+00
   4.49028350e-02  4.22686227e-02  4.19172039e-03  4.12824638e-02]
 [ 9.25097656e+00  3.99519989e+02  4.06622986e+02 -1.00000000e+00
   1.50293000e+07  3.97372009e+02  4.03556000e+02  2.41398926e+01
   2.41920052e+01 -9.75454254e+01 -7.58601074e+01  0.00000000e+00
   3.96977551e-02  4.01329473e-02 -2.64588604e-03  4.12824638e-02]
 [ 2.66849976e+01  3.77181000e+02  4.01016998e+02 -1.00000000e+00
   2.36133000e+07  3.74332001e+02  3.99471008e+02  2.54101124e+01
   2.13112183e+01 -1.17576416e+02 -7.45898895e+01  0.00000000e+00
   5.68910651e-02  4.44922335e-02 -7.11103296e-03  4.12824638e-02]
 [ 1.29710083e+01  3.75467010e+02  3.85210999e+02 -1.00000000e+00
   3.24977000e+07  3.72239990e+02  3.76928009e+02  2.62415829e+01
   2.37552738e+01 -1.14998222e+02 -7.37584152e+01  0.00000000e+00
   4.39289026e-02  4.40096073e-02 -8.95336270e-03  4.12824638e-02]
 [ 1.75339966e+01  3.86944000e+02  3.90976990e+02 -1.00000000e+00
   3.47073000e+07  3.73442993e+02  3.76088013e+02  1.53272114e+01
   1.18395910e+01 -1.46774109e+02 -8.46727905e+01  0.00000000e+00
   5.42539433e-02  2.64749769e-02 -1.65362451e-02  4.12824638e-02]
 [ 1.05989990e+01  3.83614990e+02  3.91378998e+02 -1.00000000e+00
   2.62294000e+07  3.80779999e+02  3.87427002e+02  1.34424496e+01
   1.21434402e+01 -1.35368988e+02 -8.65575485e+01  0.00000000e+00
   4.32932414e-02  2.60043256e-02 -1.36394631e-02  4.12824638e-02]
 [ 1.25509949e+01  3.75071991e+02  3.85497009e+02 -1.00000000e+00
   2.17777000e+07  3.72946014e+02  3.83988007e+02  4.08557796e+00
   1.09517460e+01 -1.36586502e+02 -9.59144211e+01  1.23718281e+01
   5.02683483e-02  2.56134793e-02 -1.28946966e-02  4.12824638e-02]
 [ 1.98359985e+01  3.59511993e+02  3.77695007e+02 -1.00000000e+00
   3.09012000e+07  3.57859009e+02  3.75181000e+02  1.97493923e+00
   6.50098896e+00 -1.59495667e+02 -9.80250626e+01  1.15652332e+01
   5.73266782e-02  2.78406814e-02 -1.63397565e-02  4.12824638e-02]
 [ 3.86010132e+01  3.28865997e+02  3.64487000e+02 -1.00000000e+00
   4.72365000e+07  3.25885986e+02  3.59891998e+02  2.57628131e+00
   2.87893295e+00 -2.12393890e+02 -9.74237213e+01  1.01750069e+01
   7.98910782e-02  3.78965698e-02 -2.67847553e-02  4.12824638e-02]
 [ 5.25050049e+01  3.20510010e+02  3.41800995e+02 -1.00000000e+00
   8.33080960e+07  2.89295990e+02  3.28915985e+02  2.05003357e+01
   8.35051918e+00 -2.27151123e+02 -7.94996643e+01  8.65160465e+00
   9.46416184e-02  3.56774963e-02 -2.24267431e-02  4.12824638e-02]
 [ 4.25740051e+01  3.30079010e+02  3.45134003e+02 -1.00000000e+00
   7.90118000e+07  3.02559998e+02  3.20389008e+02  2.67849388e+01
   1.66205177e+01 -1.63120636e+02 -7.32150650e+01  8.55646133e+00
   9.99014527e-02  4.05830555e-02 -1.75124928e-02  4.12824638e-02]
 [ 1.87650146e+01  3.36187012e+02  3.39247009e+02 -1.00000000e+00
   4.91999000e+07  3.20481995e+02  3.30584015e+02  3.19386292e+01
   2.64079685e+01 -1.34300491e+02 -6.80613708e+01  8.11129284e+00
   5.79439513e-02  3.83985564e-02 -1.92357171e-02  4.12824638e-02]
 [ 2.71760254e+01  3.52940002e+02  3.54364014e+02 -1.00000000e+00
   5.47363000e+07  3.27187988e+02  3.36115997e+02  4.74162674e+01
   3.53799477e+01 -9.00273361e+01 -5.25837326e+01  1.16774416e+01
   7.12304413e-02  4.65765074e-02 -1.08877616e-02  4.13389690e-02]
 [ 3.50390015e+01  3.65026001e+02  3.82726013e+02 -1.00000000e+00
   8.36411040e+07  3.47687012e+02  3.52747986e+02  6.02744408e+01
   4.65431137e+01 -3.49789696e+01 -3.97255592e+01  1.82146301e+01
   7.55693391e-02  4.91041057e-02 -2.81440443e-03  3.97121236e-02]
 [ 2.21039734e+01  3.61562012e+02  3.75066986e+02 -1.00000000e+00
   4.36657000e+07  3.52963013e+02  3.64687012e+02  6.15936852e+01
   5.64281311e+01 -3.49268227e+01 -3.84063148e+01  1.71171379e+01
   6.36736378e-02  4.63144146e-02  1.75640383e-03  3.86541709e-02]
 [ 1.12400208e+01  3.62299011e+02  3.67191010e+02 -1.00000000e+00
   1.33452000e+07  3.55950989e+02  3.61362000e+02  6.53440399e+01
   6.24040565e+01 -3.37528992e+01 -3.46559601e+01  1.65703812e+01
   3.79607305e-02  2.64983606e-02  1.42252175e-02  3.84152010e-02]
 [ 2.32890015e+01  3.78549011e+02  3.79433014e+02 -1.00000000e+00
   1.75528000e+07  3.56144012e+02  3.62605988e+02  8.74318085e+01
   7.14565125e+01 -5.53957748e+00 -1.25681906e+01  1.89652214e+01
   5.19136451e-02  2.18896754e-02  2.42624953e-02  3.98765840e-02]
 [ 2.83290100e+01  3.90414001e+02  3.97226013e+02 -1.00000000e+00
   3.52214000e+07  3.68897003e+02  3.77920990e+02  9.36884918e+01
   8.21547775e+01  4.08944855e+01 -6.31150770e+00  2.24800720e+01
   5.74210100e-02  2.19601430e-02  2.44750381e-02  3.54108177e-02]
 [ 2.03739929e+01  4.00869995e+02  4.11697998e+02 -1.00000000e+00
   3.84915000e+07  3.91324005e+02  3.91691986e+02  9.11537399e+01
   9.07580109e+01  9.54827957e+01 -8.84626293e+00  2.52525482e+01
   4.77215089e-02  2.18073986e-02  2.56574862e-02  3.56119573e-02]
 [ 1.34609985e+01  3.94773010e+02  4.02226990e+02 -1.00000000e+00
   2.52671000e+07  3.88765991e+02  4.00954987e+02  8.61726227e+01
   9.03382874e+01  8.33199997e+01 -1.38273773e+01  2.42465973e+01
   4.12405953e-02  2.35750787e-02  1.63658094e-02  3.52554135e-02]
 [ 2.57369995e+01  3.82556000e+02  3.98807007e+02 -1.00000000e+00
   2.69900000e+07  3.73070007e+02  3.94518005e+02  7.61915665e+01
   8.45059738e+01  5.59641495e+01 -2.38084316e+01  2.24085751e+01
   8.76150951e-02  2.78282259e-02  7.05285277e-03  3.57052796e-02]
 [ 1.00889893e+01  3.83757996e+02  3.85477997e+02 -1.00000000e+00
   1.36007000e+07  3.75389008e+02  3.82756012e+02  7.71735764e+01
   7.98459244e+01  5.05285721e+01 -2.28264256e+01  2.17136974e+01
   6.97697401e-02  2.69731507e-02  8.85738432e-03  3.56441401e-02]
 [ 1.61869812e+01  3.91441986e+02  3.95157990e+02 -1.00000000e+00
   1.14168000e+07  3.78971008e+02  3.83976013e+02  8.34512405e+01
   7.89387970e+01  7.35491409e+01 -1.65487576e+01  2.36506138e+01
   1.30090043e-01  2.70717274e-02  1.14266174e-02  3.35119925e-02]
 [ 7.48199463e+00  3.89545990e+02  3.93938995e+02 -1.00000000e+00
   5.91457000e+06  3.86457001e+02  3.91253998e+02  7.97027512e+01
   8.01091919e+01  7.43175125e+01 -2.02972450e+01  2.30666981e+01
   1.10460751e-01  2.30647288e-02  4.32717800e-03  3.35153230e-02]
 [ 1.18320007e+01  3.82845001e+02  3.90084015e+02 -1.00000000e+00
   1.64190000e+07  3.78252014e+02  3.89230988e+02  6.83684921e+01
   7.71741638e+01  5.31896095e+01 -3.16315079e+01  2.21359482e+01
   6.19906299e-02  2.07720157e-02 -2.60787713e-03  3.31007317e-02]
 [ 1.18119812e+01  3.86475006e+02  3.92645996e+02 -1.00000000e+00
   1.41889000e+07  3.80834015e+02  3.82420990e+02  7.01538391e+01
   7.27416992e+01  6.26452599e+01 -2.98461590e+01  2.21173935e+01
   7.19885677e-02  1.74571220e-02 -5.07932762e-03  3.31004784e-02]
 [ 6.32699585e+00  3.83157990e+02  3.88575989e+02 -1.00000000e+00
   1.16413000e+07  3.82248993e+02  3.86118011e+02  5.54138908e+01
   6.46454086e+01  5.39470367e+01 -4.45861092e+01  2.15993652e+01
   4.59628142e-02  1.69896502e-02 -4.13266523e-03  3.27357128e-02]
 [ 2.86010132e+01  3.58416992e+02  3.85048004e+02 -1.00000000e+00
   2.64569000e+07  3.56446991e+02  3.82962006e+02  9.28574276e+00
   4.49511566e+01 -6.17855883e+00 -9.07142563e+01  1.93886356e+01
   9.04217288e-02  2.73988917e-02 -8.93614255e-03  3.46834026e-02]
 [ 1.10400085e+01  3.58345001e+02  3.64345001e+02 -1.00000000e+00
   1.55857000e+07  3.53304993e+02  3.58591003e+02  8.63118553e+00
   2.44436054e+01 -3.78862305e+01 -9.13688126e+01  1.85973759e+01
   6.22842088e-02  2.71815583e-02 -9.41369589e-03  2.82420795e-02]
 [ 1.69839783e+01  3.47270996e+02  3.59860992e+02 -1.00000000e+00
   1.81275000e+07  3.42877014e+02  3.58610992e+02  6.38465452e+00
   8.10052776e+00 -8.19189987e+01 -9.36153488e+01  1.74195938e+01
   8.16216543e-02  2.46907696e-02 -1.66888684e-02  2.85749678e-02]
 [ 1.52900085e+01  3.54704010e+02  3.59221008e+02 -1.00000000e+00
   1.12725000e+07  3.43931000e+02  3.47487000e+02  1.71851597e+01
   1.07336664e+01 -8.35908966e+01 -8.28148422e+01  1.64119091e+01
   1.20622829e-01  2.84901932e-02 -1.29391979e-02  2.82393489e-02]
 [ 8.82299805e+00  3.52989014e+02  3.58631989e+02 -1.00000000e+00
   1.30330000e+07  3.49808990e+02  3.54777008e+02  1.46931925e+01
   1.27543354e+01 -8.93516846e+01 -8.53068085e+01  1.58423986e+01
   5.88383749e-02  2.85651404e-02 -1.11724772e-02  2.80895997e-02]
 [ 7.30502319e+00  3.57618011e+02  3.59984009e+02 -1.00000000e+00
   7.84588000e+06  3.52678986e+02  3.53214996e+02  2.48374119e+01
   1.89052544e+01 -8.20005646e+01 -7.51625900e+01  1.59224024e+01
   7.98948556e-02  2.90320087e-02 -1.06536122e-02  2.59754118e-02]
 [ 2.24900208e+01  3.35591003e+02  3.57833008e+02 -1.00000000e+00
   1.81927000e+07  3.35342987e+02  3.57088989e+02  3.90798271e-01
   1.33071337e+01 -1.31799210e+02 -9.96091995e+01  1.44814081e+01
   1.03277937e-01  3.47522087e-02 -1.82266012e-02  2.82561984e-02]
 [ 1.58410034e+01  3.45304993e+02  3.50912994e+02 -1.00000000e+00
   3.01779000e+07  3.35071991e+02  3.35709015e+02  1.70305920e+01
   1.40862675e+01 -1.16414703e+02 -8.29694061e+01  1.35511427e+01
   4.64107394e-02  3.18181328e-02 -4.86700423e-03  2.91427150e-02]
 [ 1.09030151e+01  3.38321014e+02  3.48045013e+02 -1.00000000e+00
   1.25454000e+07  3.37141998e+02  3.45009003e+02  5.40728855e+00
   7.60956001e+00 -1.15220596e+02 -9.45927124e+01  1.29352303e+01
   7.36196339e-02  3.22262570e-02 -7.72767235e-03  2.94088423e-02]
 [ 1.94739990e+01  3.25748993e+02  3.40528992e+02 -1.00000000e+00
   1.66772000e+07  3.21054993e+02  3.38649994e+02  6.44037104e+00
   9.62608433e+00 -1.37162460e+02 -9.35596313e+01  1.18952837e+01
   1.00186467e-01  3.30523737e-02 -8.62150732e-03  2.80725118e-02]
 [ 8.42398071e+00  3.25891998e+02  3.29049988e+02 -1.00000000e+00
   8.60362000e+06  3.20626007e+02  3.26075012e+02  7.31184673e+00
   6.38650227e+00 -1.32545166e+02 -9.26881561e+01  1.14658566e+01
   8.50144476e-02  3.07476912e-02 -1.16165169e-02  2.66599692e-02]
 [ 8.52102661e+00  3.27553986e+02  3.34002014e+02 -1.00000000e+00
   1.29485000e+07  3.25480988e+02  3.25569000e+02  9.61952209e+00
   7.79057980e+00 -1.09869621e+02 -9.03804779e+01  1.32309599e+01
   5.49171343e-02  3.13365310e-02 -1.01972576e-02  2.55616773e-02]
 [ 6.69000244e+00  3.30492004e+02  3.31766998e+02 -1.00000000e+00
   1.56555000e+07  3.25076996e+02  3.27161011e+02  1.45194998e+01
   1.04836226e+01 -1.01919601e+02 -8.54804993e+01  1.28207960e+01
   3.25420536e-02  3.08582298e-02 -1.07892780e-02  2.58589964e-02]
 [ 1.26859741e+01  3.39485992e+02  3.43368988e+02 -1.00000000e+00
   1.98172000e+07  3.30683014e+02  3.30683014e+02  2.92756920e+01
   1.78049049e+01 -6.64935303e+01 -7.07243118e+01  1.74868088e+01
   4.44102399e-02  2.39786524e-02  1.89752365e-03  2.64676046e-02]]
Sample y_train_debug[0]: 2.8879
INFO:root:
Current Market Regime Analysis:
INFO:root:{
  "primary_regime": "high_volatility",
  "volatility_level": 96.35698956778121,
  "trend_strength": 1.6965380678681596,
  "market_efficiency": 0.0003102959164731304
}
INFO:root:
Optimization Metric Weights:
INFO:root:{
  "val_loss": 0.2,
  "directional_accuracy": 0.2,
  "sharpe_ratio": 0.2,
  "volatility_adjusted_returns": 0.2,
  "calmar_ratio": 0.2
}
DEBUG: Entered objective function
DEBUG: Checking data types and shapes
DEBUG: Data validation passed
DEBUG: Slicing minimal data for debug run
DEBUG: X_train_debug dtype: float64
DEBUG: y_train_debug dtype: float64
DEBUG: X_train_debug min/max: -227.1511289433737 83641104.0
DEBUG: y_train_debug min/max: -5.968912991743814 15.19356972242467
DEBUG: X_train_debug sample: [[ 1.57520141e+01  4.57334015e+02  4.68174011e+02 -1.00000000e+00
   2.10568000e+07  4.52421997e+02  4.65864014e+02  3.58288165e+01
   3.60465318e+01 -9.70075853e+01 -6.41711835e+01  0.00000000e+00
   3.98665522e-02  4.99433522e-02 -6.69530139e-03  4.12824643e-02]
 [ 4.37559815e+01  4.24440002e+02  4.56859985e+02 -1.00000000e+00
   3.44832000e+07  4.13104004e+02  4.56859985e+02  2.06308553e+01
   1.92962617e+01 -1.53278622e+02 -7.93691447e+01  0.00000000e+00
   7.11107494e-02  6.94145693e-02 -1.94767670e-02  4.12824643e-02]
 [ 4.33029786e+01  3.94795990e+02  4.27834991e+02 -1.00000000e+00
   3.79197000e+07  3.84532013e+02  4.24102997e+02  1.90223493e+01
   1.73975975e+01 -1.69244724e+02 -8.09776507e+01  0.00000000e+00
   7.58998271e-02  6.92360843e-02 -1.93384210e-02  4.12824643e-02]
 [ 3.34129944e+01  4.08903992e+02  4.23295990e+02 -1.00000000e+00
   3.68636000e+07  3.89882996e+02  3.94673004e+02  2.19330223e+01
   2.02551616e+01 -1.48883425e+02 -7.80669777e+01  0.00000000e+00
   6.16736342e-02  6.53890341e-02 -1.68936271e-02  4.12824643e-02]
 [ 1.92449951e+01  3.98821014e+02  4.12425995e+02 -1.00000000e+00
   2.65801000e+07  3.93181000e+02  4.08084991e+02  1.87897494e+01
   2.39248291e+01 -1.29407159e+02 -8.12102506e+01  0.00000000e+00
   4.56246225e-02  3.77732932e-02 -1.45191125e-02  4.12824643e-02]
 [ 9.78598020e+00  4.02152008e+02  4.06915985e+02 -1.00000000e+00
   2.41276000e+07  3.97130005e+02  3.99100006e+02  3.39890968e+01
   3.84447165e+01 -8.87498649e+01 -6.60109032e+01  0.00000000e+00
   3.80639859e-02  3.93683891e-02 -5.61775016e-03  4.12824643e-02]
 [ 4.53600158e+01  4.35790985e+02  4.41557007e+02 -1.00000000e+00
   4.50995000e+07  3.96196991e+02  4.02092011e+02  1.72622451e+01
   1.57305865e+01 -1.76415755e+02 -8.27377549e+01  0.00000000e+00
   8.71937383e-02  6.64097842e-02 -2.36261486e-02  4.12824643e-02]
 [ 1.49800110e+01  4.23204987e+02  4.36112000e+02 -1.00000000e+00
   3.06277000e+07  4.21131988e+02  4.35751007e+02  3.11992721e+01
   2.78830339e+01 -1.29306908e+02 -6.88007279e+01  0.00000000e+00
   3.78784236e-02  5.65460828e-02 -9.65330629e-03  4.12824643e-02]
 [ 1.40520019e+01  4.11574005e+02  4.23519989e+02 -1.00000000e+00
   2.68144000e+07  4.09467987e+02  4.23156006e+02  2.90437799e+01
   2.76695151e+01 -1.06406378e+02 -7.09562201e+01  0.00000000e+00
   4.18227744e-02  5.05678760e-02 -3.30437996e-03  4.12824643e-02]
 [ 1.49289856e+01  4.04424988e+02  4.14937988e+02 -1.00000000e+00
   2.14608000e+07  4.00009003e+02  4.11428986e+02  2.87779568e+01
   2.83424030e+01 -7.97275846e+01 -7.12220432e+01  0.00000000e+00
   4.49028339e-02  4.22686236e-02  4.19172022e-03  4.12824643e-02]
 [ 9.25097650e+00  3.99519989e+02  4.06622986e+02 -1.00000000e+00
   1.50293000e+07  3.97372009e+02  4.03556000e+02  2.41398920e+01
   2.41920053e+01 -9.75454262e+01 -7.58601080e+01  0.00000000e+00
   3.96977559e-02  4.01329457e-02 -2.64588609e-03  4.12824643e-02]
 [ 2.66849976e+01  3.77181000e+02  4.01016998e+02 -1.00000000e+00
   2.36133000e+07  3.74332001e+02  3.99471008e+02  2.54101122e+01
   2.13112181e+01 -1.17576414e+02 -7.45898878e+01  0.00000000e+00
   5.68910662e-02  4.44922334e-02 -7.11103287e-03  4.12824643e-02]
 [ 1.29710083e+01  3.75467011e+02  3.85210999e+02 -1.00000000e+00
   3.24977000e+07  3.72239990e+02  3.76928009e+02  2.62415837e+01
   2.37552733e+01 -1.14998225e+02 -7.37584163e+01  0.00000000e+00
   4.39289025e-02  4.40096084e-02 -8.95336298e-03  4.12824643e-02]
 [ 1.75339965e+01  3.86944000e+02  3.90976990e+02 -1.00000000e+00
   3.47073000e+07  3.73442993e+02  3.76088013e+02  1.53272112e+01
   1.18395909e+01 -1.46774115e+02 -8.46727888e+01  0.00000000e+00
   5.42539419e-02  2.64749768e-02 -1.65362457e-02  4.12824643e-02]
 [ 1.05989990e+01  3.83614990e+02  3.91378998e+02 -1.00000000e+00
   2.62294000e+07  3.80779999e+02  3.87427002e+02  1.34424494e+01
   1.21434405e+01 -1.35368985e+02 -8.65575506e+01  0.00000000e+00
   4.32932415e-02  2.60043252e-02 -1.36394634e-02  4.12824643e-02]
 [ 1.25509949e+01  3.75071991e+02  3.85497009e+02 -1.00000000e+00
   2.17777000e+07  3.72946014e+02  3.83988007e+02  4.08557803e+00
   1.09517462e+01 -1.36586497e+02 -9.59144220e+01  1.23718283e+01
   5.02683498e-02  2.56134800e-02 -1.28946963e-02  4.12824643e-02]
 [ 1.98359985e+01  3.59511993e+02  3.77695007e+02 -1.00000000e+00
   3.09012000e+07  3.57859009e+02  3.75181000e+02  1.97493923e+00
   6.50098887e+00 -1.59495668e+02 -9.80250608e+01  1.15652333e+01
   5.73266799e-02  2.78406821e-02 -1.63397561e-02  4.12824643e-02]
 [ 3.86010132e+01  3.28865997e+02  3.64487000e+02 -1.00000000e+00
   4.72365000e+07  3.25885986e+02  3.59891998e+02  2.57628141e+00
   2.87893289e+00 -2.12393888e+02 -9.74237186e+01  1.01750065e+01
   7.98910801e-02  3.78965709e-02 -2.67847551e-02  4.12824643e-02]
 [ 5.25050049e+01  3.20510010e+02  3.41800995e+02 -1.00000000e+00
   8.33080960e+07  2.89295990e+02  3.28915985e+02  2.05003358e+01
   8.35051883e+00 -2.27151129e+02 -7.94996642e+01  8.65160419e+00
   9.46416192e-02  3.56774964e-02 -2.24267430e-02  4.12824643e-02]
 [ 4.25740051e+01  3.30079010e+02  3.45134003e+02 -1.00000000e+00
   7.90118000e+07  3.02559998e+02  3.20389008e+02  2.67849387e+01
   1.66205186e+01 -1.63120634e+02 -7.32150613e+01  8.55646175e+00
   9.99014546e-02  4.05830545e-02 -1.75124928e-02  4.12824643e-02]
 [ 1.87650147e+01  3.36187012e+02  3.39247009e+02 -1.00000000e+00
   4.91999000e+07  3.20481995e+02  3.30584015e+02  3.19386298e+01
   2.64079681e+01 -1.34300485e+02 -6.80613702e+01  8.11129286e+00
   5.79439514e-02  3.83985557e-02 -1.92357175e-02  4.12824643e-02]
 [ 2.71760254e+01  3.52940002e+02  3.54364014e+02 -1.00000000e+00
   5.47363000e+07  3.27187988e+02  3.36115997e+02  4.74162690e+01
   3.53799458e+01 -9.00273393e+01 -5.25837310e+01  1.16774415e+01
   7.12304431e-02  4.65765089e-02 -1.08877613e-02  4.13389677e-02]
 [ 3.50390015e+01  3.65026001e+02  3.82726013e+02 -1.00000000e+00
   8.36411040e+07  3.47687012e+02  3.52747986e+02  6.02744401e+01
   4.65431129e+01 -3.49789684e+01 -3.97255599e+01  1.82146307e+01
   7.55693409e-02  4.91041073e-02 -2.81440434e-03  3.97121217e-02]
 [ 2.21039734e+01  3.61562012e+02  3.75066986e+02 -1.00000000e+00
   4.36657000e+07  3.52963013e+02  3.64687012e+02  6.15936863e+01
   5.64281318e+01 -3.49268224e+01 -3.84063137e+01  1.71171387e+01
   6.36736354e-02  4.63144159e-02  1.75640382e-03  3.86541705e-02]
 [ 1.12400207e+01  3.62299011e+02  3.67191010e+02 -1.00000000e+00
   1.33452000e+07  3.55950989e+02  3.61362000e+02  6.53440408e+01
   6.24040557e+01 -3.37528978e+01 -3.46559592e+01  1.65703809e+01
   3.79607320e-02  2.64983614e-02  1.42252177e-02  3.84152015e-02]
 [ 2.32890014e+01  3.78549011e+02  3.79433014e+02 -1.00000000e+00
   1.75528000e+07  3.56144013e+02  3.62605987e+02  8.74318098e+01
   7.14565123e+01 -5.53957762e+00 -1.25681902e+01  1.89652207e+01
   5.19136455e-02  2.18896759e-02  2.42624951e-02  3.98765829e-02]
 [ 2.83290100e+01  3.90414001e+02  3.97226013e+02 -1.00000000e+00
   3.52214000e+07  3.68897003e+02  3.77920990e+02  9.36884923e+01
   8.21547810e+01  4.08944863e+01 -6.31150768e+00  2.24800729e+01
   5.74210109e-02  2.19601435e-02  2.44750376e-02  3.54108191e-02]
 [ 2.03739929e+01  4.00869995e+02  4.11697998e+02 -1.00000000e+00
   3.84915000e+07  3.91324005e+02  3.91691986e+02  9.11537375e+01
   9.07580132e+01  9.54827979e+01 -8.84626247e+00  2.52525476e+01
   4.77215073e-02  2.18073993e-02  2.56574868e-02  3.56119562e-02]
 [ 1.34609985e+01  3.94773010e+02  4.02226990e+02 -1.00000000e+00
   2.52671000e+07  3.88765991e+02  4.00954987e+02  8.61726225e+01
   9.03382841e+01  8.33199994e+01 -1.38273775e+01  2.42465976e+01
   4.12405951e-02  2.35750781e-02  1.63658100e-02  3.52554153e-02]
 [ 2.57369995e+01  3.82556000e+02  3.98807007e+02 -1.00000000e+00
   2.69900000e+07  3.73070007e+02  3.94518005e+02  7.61915685e+01
   8.45059762e+01  5.59641489e+01 -2.38084315e+01  2.24085753e+01
   8.76150916e-02  2.78282254e-02  7.05285274e-03  3.57052781e-02]
 [ 1.00889892e+01  3.83757996e+02  3.85477997e+02 -1.00000000e+00
   1.36007000e+07  3.75389008e+02  3.82756012e+02  7.71735751e+01
   7.98459221e+01  5.05285738e+01 -2.28264249e+01  2.17136979e+01
   6.97697420e-02  2.69731499e-02  8.85738393e-03  3.56441416e-02]
 [ 1.61869812e+01  3.91441986e+02  3.95157989e+02 -1.00000000e+00
   1.14168000e+07  3.78971008e+02  3.83976013e+02  8.34512422e+01
   7.89387953e+01  7.35491386e+01 -1.65487578e+01  2.36506146e+01
   1.30090047e-01  2.70717273e-02  1.14266174e-02  3.35119926e-02]
 [ 7.48199470e+00  3.89545990e+02  3.93938995e+02 -1.00000000e+00
   5.91457000e+06  3.86457001e+02  3.91253998e+02  7.97027544e+01
   8.01091906e+01  7.43175129e+01 -2.02972456e+01  2.30666975e+01
   1.10460750e-01  2.30647288e-02  4.32717821e-03  3.35153243e-02]
 [ 1.18320007e+01  3.82845001e+02  3.90084015e+02 -1.00000000e+00
   1.64190000e+07  3.78252014e+02  3.89230987e+02  6.83684927e+01
   7.71741631e+01  5.31896102e+01 -3.16315073e+01  2.21359488e+01
   6.19906297e-02  2.07720161e-02 -2.60787719e-03  3.31007298e-02]
 [ 1.18119812e+01  3.86475006e+02  3.92645996e+02 -1.00000000e+00
   1.41889000e+07  3.80834015e+02  3.82420990e+02  7.01538410e+01
   7.27416960e+01  6.26452608e+01 -2.98461590e+01  2.21173943e+01
   7.19885700e-02  1.74571216e-02 -5.07932764e-03  3.31004786e-02]
 [ 6.32699590e+00  3.83157989e+02  3.88575989e+02 -1.00000000e+00
   1.16413000e+07  3.82248993e+02  3.86118012e+02  5.54138904e+01
   6.46454080e+01  5.39470386e+01 -4.45861096e+01  2.15993655e+01
   4.59628154e-02  1.69896504e-02 -4.13266531e-03  3.27357130e-02]
 [ 2.86010132e+01  3.58416992e+02  3.85048004e+02 -1.00000000e+00
   2.64569000e+07  3.56446991e+02  3.82962006e+02  9.28574251e+00
   4.49511580e+01 -6.17855864e+00 -9.07142575e+01  1.93886351e+01
   9.04217284e-02  2.73988922e-02 -8.93614300e-03  3.46834025e-02]
 [ 1.10400085e+01  3.58345001e+02  3.64345001e+02 -1.00000000e+00
   1.55857000e+07  3.53304993e+02  3.58591003e+02  8.63118532e+00
   2.44436061e+01 -3.78862304e+01 -9.13688147e+01  1.85973753e+01
   6.22842081e-02  2.71815584e-02 -9.41369598e-03  2.82420796e-02]
 [ 1.69839782e+01  3.47270996e+02  3.59860992e+02 -1.00000000e+00
   1.81275000e+07  3.42877014e+02  3.58610992e+02  6.38465430e+00
   8.10052738e+00 -8.19189988e+01 -9.36153457e+01  1.74195941e+01
   8.16216507e-02  2.46907696e-02 -1.66888681e-02  2.85749682e-02]
 [ 1.52900085e+01  3.54704010e+02  3.59221008e+02 -1.00000000e+00
   1.12725000e+07  3.43931000e+02  3.47487000e+02  1.71851594e+01
   1.07336664e+01 -8.35908985e+01 -8.28148406e+01  1.64119086e+01
   1.20622826e-01  2.84901934e-02 -1.29391977e-02  2.82393492e-02]
 [ 8.82299800e+00  3.52989014e+02  3.58631988e+02 -1.00000000e+00
   1.30330000e+07  3.49808990e+02  3.54777008e+02  1.46931923e+01
   1.27543354e+01 -8.93516879e+01 -8.53068077e+01  1.58423989e+01
   5.88383763e-02  2.85651409e-02 -1.11724775e-02  2.80895988e-02]
 [ 7.30502320e+00  3.57618012e+02  3.59984009e+02 -1.00000000e+00
   7.84588000e+06  3.52678986e+02  3.53214996e+02  2.48374109e+01
   1.89052542e+01 -8.20005636e+01 -7.51625891e+01  1.59224023e+01
   7.98948592e-02  2.90320083e-02 -1.06536118e-02  2.59754114e-02]
 [ 2.24900207e+01  3.35591003e+02  3.57833008e+02 -1.00000000e+00
   1.81927000e+07  3.35342987e+02  3.57088989e+02  3.90798284e-01
   1.33071338e+01 -1.31799207e+02 -9.96092017e+01  1.44814086e+01
   1.03277939e-01  3.47522073e-02 -1.82266020e-02  2.82561978e-02]
 [ 1.58410034e+01  3.45304993e+02  3.50912994e+02 -1.00000000e+00
   3.01779000e+07  3.35071991e+02  3.35709015e+02  1.70305927e+01
   1.40862673e+01 -1.16414704e+02 -8.29694073e+01  1.35511430e+01
   4.64107395e-02  3.18181341e-02 -4.86700412e-03  2.91427160e-02]
 [ 1.09030151e+01  3.38321014e+02  3.48045013e+02 -1.00000000e+00
   1.25454000e+07  3.37141998e+02  3.45009003e+02  5.40728869e+00
   7.60955990e+00 -1.15220598e+02 -9.45927113e+01  1.29352306e+01
   7.36196329e-02  3.22262562e-02 -7.72767226e-03  2.94088417e-02]
 [ 1.94739990e+01  3.25748993e+02  3.40528992e+02 -1.00000000e+00
   1.66772000e+07  3.21054993e+02  3.38649994e+02  6.44037104e+00
   9.62608416e+00 -1.37162464e+02 -9.35596290e+01  1.18952841e+01
   1.00186467e-01  3.30523732e-02 -8.62150701e-03  2.80725116e-02]
 [ 8.42398070e+00  3.25891998e+02  3.29049988e+02 -1.00000000e+00
   8.60362000e+06  3.20626007e+02  3.26075012e+02  7.31184672e+00
   6.38650215e+00 -1.32545165e+02 -9.26881533e+01  1.14658562e+01
   8.50144476e-02  3.07476911e-02 -1.16165168e-02  2.66599688e-02]
 [ 8.52102670e+00  3.27553986e+02  3.34002014e+02 -1.00000000e+00
   1.29485000e+07  3.25480987e+02  3.25569000e+02  9.61952174e+00
   7.79057983e+00 -1.09869623e+02 -9.03804783e+01  1.32309598e+01
   5.49171325e-02  3.13365327e-02 -1.01972572e-02  2.55616766e-02]
 [ 6.69000250e+00  3.30492004e+02  3.31766998e+02 -1.00000000e+00
   1.56555000e+07  3.25076996e+02  3.27161011e+02  1.45194996e+01
   1.04836227e+01 -1.01919602e+02 -8.54805004e+01  1.28207962e+01
   3.25420539e-02  3.08582298e-02 -1.07892780e-02  2.58589970e-02]
 [ 1.26859741e+01  3.39485992e+02  3.43368988e+02 -1.00000000e+00
   1.98172000e+07  3.30683014e+02  3.30683014e+02  2.92756918e+01
   1.78049044e+01 -6.64935281e+01 -7.07243082e+01  1.74868079e+01
   4.44102385e-02  2.39786532e-02  1.89752369e-03  2.64676049e-02]]
DEBUG: y_train_debug sample: 2.887900036961888
DEBUG: X_train_debug shape before reshape: (32, 50, 16)
DEBUG: y_train_debug shape before reshape: (32,)
DEBUG: X_train_debug shape after reshape: (32, 50, 16)
DEBUG: y_train_debug shape after reshape: (32,)
DEBUG: y_train_debug shape after column selection: (32,)
DEBUG: y_val_debug shape after column selection: (8,)
DEBUG: X_train_debug min/max after scaling: -2.7157834853946174e-06 1.0
DEBUG: y_train_debug min/max after scaling: -0.3928578405727856 1.0
Any NaNs in X_train_debug? False
Any Infs in X_train_debug? False
Any NaNs in y_train_debug? False
Any Infs in y_train_debug? False
DEBUG: Trying dummy model fit
4/4 - 0s - 39ms/step - loss: 0.0681
DEBUG: Dummy model tiny fit successful
DummyTrial.suggest_categorical(batch_size, [16, 32, 64, 128]) -> 16
DummyTrial.suggest_int(epochs, 50, 200) -> 50
DummyTrial.suggest_int(patience, 10, 30) -> 10
DEBUG: Hyperparameters selected 16 50 10
DEBUG: Starting full model.fit
Epoch 1/50
191/191 ━━━━━━━━━━━━━━━━━━━━ 2s 8ms/step - loss: 14.9020 - val_loss: 6.4862
Epoch 2/50
191/191 ━━━━━━━━━━━━━━━━━━━━ 2s 8ms/step - loss: 15.0380 - val_loss: 6.4787
Epoch 3/50
191/191 ━━━━━━━━━━━━━━━━━━━━ 2s 8ms/step - loss: 15.0091 - val_loss: 6.4826
Epoch 4/50
191/191 ━━━━━━━━━━━━━━━━━━━━ 2s 8ms/step - loss: 15.6147 - val_loss: 6.4780
Epoch 5/50
191/191 ━━━━━━━━━━━━━━━━━━━━ 2s 8ms/step - loss: 14.5581 - val_loss: 6.4763
Epoch 6/50
191/191 ━━━━━━━━━━━━━━━━━━━━ 2s 8ms/step - loss: 13.0584 - val_loss: 6.4829
Epoch 7/50
191/191 ━━━━━━━━━━━━━━━━━━━━ 2s 8ms/step - loss: 14.5880 - val_loss: 6.4769
Epoch 8/50
191/191 ━━━━━━━━━━━━━━━━━━━━ 2s 8ms/step - loss: 15.4968 - val_loss: 6.4827
Epoch 9/50
191/191 ━━━━━━━━━━━━━━━━━━━━ 2s 8ms/step - loss: 14.7192 - val_loss: 6.4759
Epoch 10/50
191/191 ━━━━━━━━━━━━━━━━━━━━ 2s 8ms/step - loss: 14.3483 - val_loss: 6.4774
Epoch 11/50
191/191 ━━━━━━━━━━━━━━━━━━━━ 2s 8ms/step - loss: 12.8436 - val_loss: 6.4812
Epoch 12/50
191/191 ━━━━━━━━━━━━━━━━━━━━ 2s 8ms/step - loss: 13.6247 - val_loss: 6.4764
Epoch 13/50
191/191 ━━━━━━━━━━━━━━━━━━━━ 2s 8ms/step - loss: 13.7184 - val_loss: 6.4828
Epoch 14/50
191/191 ━━━━━━━━━━━━━━━━━━━━ 2s 8ms/step - loss: 15.5842 - val_loss: 6.4843
Epoch 15/50
191/191 ━━━━━━━━━━━━━━━━━━━━ 2s 8ms/step - loss: 14.1479 - val_loss: 6.4817
Epoch 16/50
191/191 ━━━━━━━━━━━━━━━━━━━━ 2s 8ms/step - loss: 14.1183 - val_loss: 6.4800
Epoch 17/50
191/191 ━━━━━━━━━━━━━━━━━━━━ 2s 8ms/step - loss: 16.9046 - val_loss: 6.4834
Epoch 18/50
191/191 ━━━━━━━━━━━━━━━━━━━━ 2s 8ms/step - loss: 14.5425 - val_loss: 6.4776
Epoch 19/50
191/191 ━━━━━━━━━━━━━━━━━━━━ 2s 8ms/step - loss: 14.1260 - val_loss: 6.4764
DEBUG: model.fit finished
DEBUG: Predicting on validation set
DEBUG: Metrics calculated
DEBUG: Weighted score calculated
DEBUG: MLflow logging complete
DummyTrial.set_user_attr(val_loss, 6.475872993469238)
DummyTrial.set_user_attr(directional_accuracy, 0.4927916120576671)
DummyTrial.set_user_attr(sharpe_ratio, -173156814.12840685)
DummyTrial.set_user_attr(volatility_adjusted_returns, 0)
DummyTrial.set_user_attr(calmar_ratio, 0)
DummyTrial.set_user_attr(market_regime, high_volatility)
DummyTrial.set_user_attr(regime_volatility, 96.35698956778121)
DEBUG: Optuna trial attributes set
Objective function result: -34631364.02229765


Best LGBM params: {'colsample_bytree': 0.7, 'learning_rate': 0.01, 'max_depth': 5, 'min_child_samples': 20, 'n_estimators': 200, 'num_leaves': 31, 'subsample': 0.7}
Best LGBM RMSE: 1.722807925634613

→ Diversity-based Ensemble (RNN + XGB + LGBM):
122/122 ━━━━━━━━━━━━━━━━━━━━ 11s 88ms/step 
3-model Ensemble RMSE: 1.7773
Saved ensemble_3model_predictions2.csv

✅ Hyperparameter tuning complete.

----------------------------------------------------------------------------------------------------------------------------------------------------

Best LGBM params: {'colsample_bytree': 1.0, 'learning_rate': 0.05, 'max_depth': 5, 'min_child_samples': 20, 'n_estimators': 200, 'num_leaves': 31, 'subsample': 0.7}
Best LGBM RMSE: 2.155572873639039

→ Diversity-based Ensemble (RNN + XGB + LGBM):
122/122 ━━━━━━━━━━━━━━━━━━━━ 9s 72ms/step 
3-model Ensemble RMSE: 1.1802
Saved ensemble_3model_predictions1.csv

✅ Hyperparameter tuning complete.

Epoch 3/5
97/97 - 17s - 178ms/step - RootMeanSquaredError: 3.7795 - loss: 14.2846 - val_RootMeanSquaredError: 2.5192 - val_loss: 6.3464
Epoch 4/5
97/97 - 17s - 178ms/step - RootMeanSquaredError: 3.7569 - loss: 14.1141 - val_RootMeanSquaredError: 2.4997 - val_loss: 6.2484
Epoch 5/5
97/97 - 17s - 180ms/step - RootMeanSquaredError: 3.7348 - loss: 13.9487 - val_RootMeanSquaredError: 2.4829 - val_loss: 6.1647
INFO:root:Trial 0: RMSE 2.4828768399508663
[I 2025-04-30 16:02:30,600] Trial 0 finished with value: 2.4828768399508663 and parameters: {'batch_size': 32, 'model_type': 'BiLSTM', 'n_layers': 2, 'units': 30, 'dropout': 0.21464652671823947, 'lr': 0.00013383585756044552}. Best is trial 0 with value: 2.4828768399508663.
  0%|                                                                                      | 0/75 [01:29<?, ?it/s]
Memory usage: 772.34 MB
Best trial: 0. Best value: 2.48288:   1%|▎                   | 1/75 [01:29<1:50:20, 89.47s/it, 89.47/5400 seconds]INFO:root:Model created: type=LSTM, layers=2, units=64, dropout=0.017335290738181186, lr=0.0014896817352330416
Epoch 1/5
97/97 - 7s - 69ms/step - RootMeanSquaredError: 3.7289 - loss: 13.9046 - val_RootMeanSquaredError: 2.4096 - val_loss: 5.8061
Epoch 2/5
97/97 - 6s - 62ms/step - RootMeanSquaredError: 3.4782 - loss: 12.0980 - val_RootMeanSquaredError: 2.0881 - val_loss: 4.3604
Epoch 3/5
97/97 - 6s - 62ms/step - RootMeanSquaredError: 3.0978 - loss: 9.5963 - val_RootMeanSquaredError: 1.7391 - val_loss: 3.0245
Epoch 4/5
97/97 - 6s - 62ms/step - RootMeanSquaredError: 2.7542 - loss: 7.5857 - val_RootMeanSquaredError: 1.8476 - val_loss: 3.4136
Epoch 5/5
97/97 - 6s - 63ms/step - RootMeanSquaredError: 2.4983 - loss: 6.2416 - val_RootMeanSquaredError: 1.6170 - val_loss: 2.6148
INFO:root:Trial 1: RMSE 1.6170454208650955
[I 2025-04-30 16:03:02,120] Trial 1 finished with value: 1.6170454208650955 and parameters: {'batch_size': 32, 'model_type': 'LSTM', 'n_layers': 2, 'units': 64, 'dropout': 0.017335290738181186, 'lr': 0.0014896817352330416}. Best is trial 1 with value: 1.6170454208650955.
Best trial: 0. Best value: 2.48288:   1%|▎                   | 1/75 [02:00<1:50:20, 89.47s/it, 89.47/5400 seconds]
Memory usage: 828.98 MB
Best trial: 1. Best value: 1.61705:   3%|▌                  | 2/75 [02:00<1:07:22, 55.38s/it, 120.99/5400 seconds]INFO:root:Model created: type=GRU, layers=2, units=64, dropout=0.3477114701350227, lr=0.001474218727785943
Epoch 1/5
194/194 - 12s - 62ms/step - RootMeanSquaredError: 3.6194 - loss: 13.0997 - val_RootMeanSquaredError: 2.1833 - val_loss: 4.7668
Epoch 2/5
194/194 - 11s - 59ms/step - RootMeanSquaredError: 3.2135 - loss: 10.3267 - val_RootMeanSquaredError: 1.8515 - val_loss: 3.4282
Epoch 3/5
194/194 - 11s - 59ms/step - RootMeanSquaredError: 2.7977 - loss: 7.8272 - val_RootMeanSquaredError: 1.7857 - val_loss: 3.1889
Epoch 4/5
194/194 - 11s - 59ms/step - RootMeanSquaredError: 2.6463 - loss: 7.0027 - val_RootMeanSquaredError: 1.8743 - val_loss: 3.5130
Epoch 5/5
194/194 - 11s - 59ms/step - RootMeanSquaredError: 2.4765 - loss: 6.1331 - val_RootMeanSquaredError: 1.7331 - val_loss: 3.0038
INFO:root:Trial 2: RMSE 1.7331474949415153
[I 2025-04-30 16:04:00,519] Trial 2 finished with value: 1.7331474949415153 and parameters: {'batch_size': 16, 'model_type': 'GRU', 'n_layers': 2, 'units': 64, 'dropout': 0.3477114701350227, 'lr': 0.001474218727785943}. Best is trial 1 with value: 1.6170454208650955.
Best trial: 1. Best value: 1.61705:   3%|▌                  | 2/75 [02:59<1:07:22, 55.38s/it, 120.99/5400 seconds]
Memory usage: 882.70 MB
Best trial: 1. Best value: 1.61705:   4%|▊                  | 3/75 [02:59<1:08:06, 56.76s/it, 179.39/5400 seconds]INFO:root:Model created: type=BiLSTM, layers=2, units=15, dropout=0.482572699062353, lr=0.003058403904627845
Epoch 1/5
97/97 - 18s - 190ms/step - RootMeanSquaredError: 3.7567 - loss: 14.1130 - val_RootMeanSquaredError: 2.3969 - val_loss: 5.7449
Epoch 2/5
97/97 - 18s - 183ms/step - RootMeanSquaredError: 3.5354 - loss: 12.4990 - val_RootMeanSquaredError: 2.1455 - val_loss: 4.6034
Epoch 3/5
97/97 - 18s - 181ms/step - RootMeanSquaredError: 3.2336 - loss: 10.4561 - val_RootMeanSquaredError: 1.7154 - val_loss: 2.9426
Epoch 4/5
97/97 - 18s - 182ms/step - RootMeanSquaredError: 2.8949 - loss: 8.3803 - val_RootMeanSquaredError: 1.5995 - val_loss: 2.5584
Epoch 5/5
97/97 - 18s - 184ms/step - RootMeanSquaredError: 2.6074 - loss: 6.7987 - val_RootMeanSquaredError: 1.5344 - val_loss: 2.3544
INFO:root:Trial 3: RMSE 1.5343956392949498
[I 2025-04-30 16:05:31,811] Trial 3 finished with value: 1.5343956392949498 and parameters: {'batch_size': 32, 'model_type': 'BiLSTM', 'n_layers': 2, 'units': 15, 'dropout': 0.482572699062353, 'lr': 0.003058403904627845}. Best is trial 3 with value: 1.5343956392949498.
Best trial: 1. Best value: 1.61705:   4%|▊                  | 3/75 [04:30<1:08:06, 56.76s/it, 179.39/5400 seconds]
Memory usage: 966.66 MB
Best trial: 3. Best value: 1.5344:   5%|█                   | 4/75 [04:30<1:23:17, 70.39s/it, 270.68/5400 seconds]INFO:root:Model created: type=LSTM, layers=2, units=29, dropout=0.0571741746842801, lr=0.0001762489520052978
Epoch 1/5
97/97 - 7s - 68ms/step - RootMeanSquaredError: 3.8132 - loss: 14.5403 - val_RootMeanSquaredError: 2.5616 - val_loss: 6.5616
Epoch 2/5
97/97 - 7s - 75ms/step - RootMeanSquaredError: 3.8043 - loss: 14.4727 - val_RootMeanSquaredError: 2.5494 - val_loss: 6.4992
Epoch 3/5
97/97 - 6s - 63ms/step - RootMeanSquaredError: 3.7881 - loss: 14.3500 - val_RootMeanSquaredError: 2.5288 - val_loss: 6.3947
Epoch 4/5
97/97 - 7s - 70ms/step - RootMeanSquaredError: 3.7624 - loss: 14.1555 - val_RootMeanSquaredError: 2.5057 - val_loss: 6.2788
Epoch 5/5
97/97 - 8s - 78ms/step - RootMeanSquaredError: 3.7392 - loss: 13.9815 - val_RootMeanSquaredError: 2.4906 - val_loss: 6.2032
INFO:root:Trial 4: RMSE 2.4906164934136092
[I 2025-04-30 16:06:06,888] Trial 4 finished with value: 2.4906164934136092 and parameters: {'batch_size': 32, 'model_type': 'LSTM', 'n_layers': 2, 'units': 29, 'dropout': 0.0571741746842801, 'lr': 0.0001762489520052978}. Best is trial 3 with value: 1.5343956392949498.
Best trial: 3. Best value: 1.5344:   5%|█                   | 4/75 [05:05<1:23:17, 70.39s/it, 270.68/5400 seconds]
Memory usage: 939.52 MB
Best trial: 3. Best value: 1.5344:   7%|█▎                  | 5/75 [05:05<1:07:16, 57.66s/it, 305.76/5400 seconds]INFO:root:Model created: type=BiLSTM, layers=1, units=18, dropout=0.2788430000946793, lr=0.0013420379093979104
Epoch 1/5
97/97 - 10s - 101ms/step - RootMeanSquaredError: 3.7655 - loss: 14.1789 - val_RootMeanSquaredError: 2.4825 - val_loss: 6.1628
Epoch 2/5
97/97 - 9s - 90ms/step - RootMeanSquaredError: 3.6832 - loss: 13.5661 - val_RootMeanSquaredError: 2.3894 - val_loss: 5.7093
Epoch 3/5
97/97 - 9s - 90ms/step - RootMeanSquaredError: 3.5717 - loss: 12.7570 - val_RootMeanSquaredError: 2.2899 - val_loss: 5.2437
Epoch 4/5
97/97 - 10s - 102ms/step - RootMeanSquaredError: 3.4724 - loss: 12.0575 - val_RootMeanSquaredError: 2.2002 - val_loss: 4.8409
Epoch 5/5
97/97 - 10s - 98ms/step - RootMeanSquaredError: 3.3809 - loss: 11.4308 - val_RootMeanSquaredError: 2.1275 - val_loss: 4.5262
INFO:root:Trial 5: RMSE 2.1274800189245204
[I 2025-04-30 16:06:54,857] Trial 5 finished with value: 2.1274800189245204 and parameters: {'batch_size': 32, 'model_type': 'BiLSTM', 'n_layers': 1, 'units': 18, 'dropout': 0.2788430000946793, 'lr': 0.0013420379093979104}. Best is trial 3 with value: 1.5343956392949498.
Best trial: 3. Best value: 1.5344:   7%|█▎                  | 5/75 [05:53<1:07:16, 57.66s/it, 305.76/5400 seconds]
Memory usage: 1011.77 MB
Best trial: 3. Best value: 1.5344:   8%|█▌                  | 6/75 [05:53<1:02:31, 54.36s/it, 353.73/5400 seconds]INFO:root:Model created: type=GRU, layers=1, units=18, dropout=0.17597119067965145, lr=0.00885153450656345
Epoch 1/5
194/194 - 9s - 49ms/step - RootMeanSquaredError: 3.4782 - loss: 12.0976 - val_RootMeanSquaredError: 2.2181 - val_loss: 4.9198
Epoch 2/5
194/194 - 8s - 39ms/step - RootMeanSquaredError: 3.0027 - loss: 9.0163 - val_RootMeanSquaredError: 1.7347 - val_loss: 3.0093
Epoch 3/5
194/194 - 8s - 39ms/step - RootMeanSquaredError: 2.6801 - loss: 7.1829 - val_RootMeanSquaredError: 1.6550 - val_loss: 2.7392
Epoch 4/5
194/194 - 8s - 39ms/step - RootMeanSquaredError: 2.4897 - loss: 6.1985 - val_RootMeanSquaredError: 1.5631 - val_loss: 2.4432
Epoch 5/5
194/194 - 8s - 39ms/step - RootMeanSquaredError: 2.4140 - loss: 5.8273 - val_RootMeanSquaredError: 1.8928 - val_loss: 3.5828
INFO:root:Trial 6: RMSE 1.5630819557324056
[I 2025-04-30 16:07:35,182] Trial 6 finished with value: 1.5630819557324056 and parameters: {'batch_size': 16, 'model_type': 'GRU', 'n_layers': 1, 'units': 18, 'dropout': 0.17597119067965145, 'lr': 0.00885153450656345}. Best is trial 3 with value: 1.5343956392949498.
Best trial: 3. Best value: 1.5344:   8%|█▌                  | 6/75 [06:34<1:02:31, 54.36s/it, 353.73/5400 seconds]
Memory usage: 1073.69 MB
Best trial: 3. Best value: 1.5344:   9%|██                    | 7/75 [06:34<56:24, 49.77s/it, 394.05/5400 seconds]INFO:root:Model created: type=BiLSTM, layers=2, units=24, dropout=0.1711689323652964, lr=0.007919125033941954
Epoch 1/5
97/97 - 18s - 189ms/step - RootMeanSquaredError: 3.5000 - loss: 12.2501 - val_RootMeanSquaredError: 1.8889 - val_loss: 3.5680
Epoch 2/5
97/97 - 18s - 184ms/step - RootMeanSquaredError: 2.5555 - loss: 6.5308 - val_RootMeanSquaredError: 1.8654 - val_loss: 3.4799
Epoch 3/5
97/97 - 18s - 188ms/step - RootMeanSquaredError: 2.2242 - loss: 4.9472 - val_RootMeanSquaredError: 1.3695 - val_loss: 1.8755
Epoch 4/5
97/97 - 18s - 187ms/step - RootMeanSquaredError: 2.1209 - loss: 4.4982 - val_RootMeanSquaredError: 1.3925 - val_loss: 1.9391
Epoch 5/5
97/97 - 18s - 187ms/step - RootMeanSquaredError: 2.0979 - loss: 4.4012 - val_RootMeanSquaredError: 1.2638 - val_loss: 1.5971
INFO:root:Trial 7: RMSE 1.2637627608092945
[I 2025-04-30 16:09:08,177] Trial 7 finished with value: 1.2637627608092945 and parameters: {'batch_size': 32, 'model_type': 'BiLSTM', 'n_layers': 2, 'units': 24, 'dropout': 0.1711689323652964, 'lr': 0.007919125033941954}. Best is trial 7 with value: 1.2637627608092945.
Best trial: 3. Best value: 1.5344:   9%|██                    | 7/75 [08:07<56:24, 49.77s/it, 394.05/5400 seconds]
Memory usage: 1187.89 MB
Best trial: 7. Best value: 1.26376:  11%|██                 | 8/75 [08:07<1:10:56, 63.53s/it, 487.05/5400 seconds]INFO:root:Model created: type=GRU, layers=2, units=33, dropout=0.11873881006080789, lr=0.009490598160043948
Epoch 1/5
97/97 - 6s - 67ms/step - RootMeanSquaredError: 3.4969 - loss: 12.2281 - val_RootMeanSquaredError: 1.8559 - val_loss: 3.4443
Epoch 2/5
97/97 - 6s - 60ms/step - RootMeanSquaredError: 2.6404 - loss: 6.9715 - val_RootMeanSquaredError: 1.7603 - val_loss: 3.0987
Epoch 3/5
97/97 - 6s - 60ms/step - RootMeanSquaredError: 2.2928 - loss: 5.2568 - val_RootMeanSquaredError: 1.8766 - val_loss: 3.5217
Epoch 4/5
97/97 - 6s - 60ms/step - RootMeanSquaredError: 2.2251 - loss: 4.9509 - val_RootMeanSquaredError: 1.5412 - val_loss: 2.3753
Epoch 5/5
97/97 - 6s - 59ms/step - RootMeanSquaredError: 2.0390 - loss: 4.1573 - val_RootMeanSquaredError: 1.3631 - val_loss: 1.8580
INFO:root:Trial 8: RMSE 1.3630777981554965
[I 2025-04-30 16:09:38,411] Trial 8 finished with value: 1.3630777981554965 and parameters: {'batch_size': 32, 'model_type': 'GRU', 'n_layers': 2, 'units': 33, 'dropout': 0.11873881006080789, 'lr': 0.009490598160043948}. Best is trial 7 with value: 1.2637627608092945.
Best trial: 7. Best value: 1.26376:  11%|██                 | 8/75 [08:37<1:10:56, 63.53s/it, 487.05/5400 seconds]
Memory usage: 1242.95 MB
Best trial: 7. Best value: 1.26376:  12%|██▌                  | 9/75 [08:37<58:26, 53.12s/it, 517.28/5400 seconds]INFO:root:Model created: type=LSTM, layers=1, units=23, dropout=0.3219130651111729, lr=0.0032429977485253316
Epoch 1/5
194/194 - 9s - 46ms/step - RootMeanSquaredError: 3.6960 - loss: 13.6601 - val_RootMeanSquaredError: 2.3024 - val_loss: 5.3013
Epoch 2/5
194/194 - 8s - 41ms/step - RootMeanSquaredError: 3.4119 - loss: 11.6412 - val_RootMeanSquaredError: 2.1055 - val_loss: 4.4330
Epoch 3/5
194/194 - 8s - 40ms/step - RootMeanSquaredError: 3.1516 - loss: 9.9323 - val_RootMeanSquaredError: 1.8942 - val_loss: 3.5879
Epoch 4/5
194/194 - 8s - 41ms/step - RootMeanSquaredError: 2.8956 - loss: 8.3843 - val_RootMeanSquaredError: 1.6969 - val_loss: 2.8795
Epoch 5/5
194/194 - 8s - 40ms/step - RootMeanSquaredError: 2.7530 - loss: 7.5792 - val_RootMeanSquaredError: 1.6901 - val_loss: 2.8565
INFO:root:Trial 9: RMSE 1.690122810528614
[I 2025-04-30 16:10:19,175] Trial 9 finished with value: 1.690122810528614 and parameters: {'batch_size': 16, 'model_type': 'LSTM', 'n_layers': 1, 'units': 23, 'dropout': 0.3219130651111729, 'lr': 0.0032429977485253316}. Best is trial 7 with value: 1.2637627608092945.
Best trial: 7. Best value: 1.26376:  12%|██▌                  | 9/75 [09:18<58:26, 53.12s/it, 517.28/5400 seconds]
Memory usage: 1319.55 MB
Best trial: 7. Best value: 1.26376:  13%|██▋                 | 10/75 [09:18<53:25, 49.31s/it, 558.05/5400 seconds]INFO:root:Model created: type=BiLSTM, layers=1, units=9, dropout=0.4263172653414672, lr=0.00037283601907022406
Epoch 1/5
194/194 - 16s - 83ms/step - RootMeanSquaredError: 3.8049 - loss: 14.4772 - val_RootMeanSquaredError: 2.5405 - val_loss: 6.4540
Epoch 2/5
194/194 - 15s - 79ms/step - RootMeanSquaredError: 3.7837 - loss: 14.3167 - val_RootMeanSquaredError: 2.5221 - val_loss: 6.3609
Epoch 3/5
194/194 - 15s - 80ms/step - RootMeanSquaredError: 3.7694 - loss: 14.2087 - val_RootMeanSquaredError: 2.5039 - val_loss: 6.2693
Epoch 4/5
194/194 - 15s - 79ms/step - RootMeanSquaredError: 3.7416 - loss: 13.9997 - val_RootMeanSquaredError: 2.4813 - val_loss: 6.1568
Epoch 5/5
194/194 - 15s - 80ms/step - RootMeanSquaredError: 3.7135 - loss: 13.7898 - val_RootMeanSquaredError: 2.4556 - val_loss: 6.0300
INFO:root:Trial 10: RMSE 2.4556066145509488
[I 2025-04-30 16:11:38,373] Trial 10 finished with value: 2.4556066145509488 and parameters: {'batch_size': 16, 'model_type': 'BiLSTM', 'n_layers': 1, 'units': 9, 'dropout': 0.4263172653414672, 'lr': 0.00037283601907022406}. Best is trial 7 with value: 1.2637627608092945.
Best trial: 7. Best value: 1.26376:  13%|██▋                 | 10/75 [10:37<53:25, 49.31s/it, 558.05/5400 seconds]
Memory usage: 1420.67 MB
Best trial: 7. Best value: 1.26376:  15%|██▋               | 11/75 [10:37<1:02:21, 58.46s/it, 637.25/5400 seconds]INFO:root:Model created: type=GRU, layers=2, units=41, dropout=0.1163634774577176, lr=0.009957173485482584
Epoch 1/5
97/97 - 6s - 66ms/step - RootMeanSquaredError: 3.4372 - loss: 11.8146 - val_RootMeanSquaredError: 1.8799 - val_loss: 3.5342
Epoch 2/5
97/97 - 6s - 59ms/step - RootMeanSquaredError: 2.5914 - loss: 6.7156 - val_RootMeanSquaredError: 2.2930 - val_loss: 5.2578
Epoch 3/5
97/97 - 6s - 59ms/step - RootMeanSquaredError: 2.2351 - loss: 4.9957 - val_RootMeanSquaredError: 2.0060 - val_loss: 4.0242
INFO:root:Trial 11: RMSE 1.8799419604971082
[I 2025-04-30 16:11:56,852] Trial 11 finished with value: 1.8799419604971082 and parameters: {'batch_size': 32, 'model_type': 'GRU', 'n_layers': 2, 'units': 41, 'dropout': 0.1163634774577176, 'lr': 0.009957173485482584}. Best is trial 7 with value: 1.2637627608092945.
Best trial: 7. Best value: 1.26376:  15%|██▋               | 11/75 [10:55<1:02:21, 58.46s/it, 637.25/5400 seconds]
Memory usage: 1453.14 MB
Best trial: 7. Best value: 1.26376:  16%|███▏                | 12/75 [10:55<48:36, 46.29s/it, 655.72/5400 seconds]INFO:root:Model created: type=GRU, layers=2, units=40, dropout=0.1371450241774758, lr=0.004626229480716622
Epoch 1/5
97/97 - 6s - 66ms/step - RootMeanSquaredError: 3.5098 - loss: 12.3185 - val_RootMeanSquaredError: 2.1612 - val_loss: 4.6708
Epoch 2/5
97/97 - 6s - 60ms/step - RootMeanSquaredError: 2.9069 - loss: 8.4501 - val_RootMeanSquaredError: 1.9244 - val_loss: 3.7032
Epoch 3/5
97/97 - 6s - 59ms/step - RootMeanSquaredError: 2.5312 - loss: 6.4068 - val_RootMeanSquaredError: 1.5708 - val_loss: 2.4674
Epoch 4/5
97/97 - 6s - 59ms/step - RootMeanSquaredError: 2.3744 - loss: 5.6377 - val_RootMeanSquaredError: 1.5152 - val_loss: 2.2958
Epoch 5/5
97/97 - 6s - 59ms/step - RootMeanSquaredError: 2.2038 - loss: 4.8566 - val_RootMeanSquaredError: 1.4671 - val_loss: 2.1525
INFO:root:Trial 12: RMSE 1.4671343188693533
[I 2025-04-30 16:12:26,752] Trial 12 finished with value: 1.4671343188693533 and parameters: {'batch_size': 32, 'model_type': 'GRU', 'n_layers': 2, 'units': 40, 'dropout': 0.1371450241774758, 'lr': 0.004626229480716622}. Best is trial 7 with value: 1.2637627608092945.
Best trial: 7. Best value: 1.26376:  16%|███▏                | 12/75 [11:25<48:36, 46.29s/it, 655.72/5400 seconds]
Memory usage: 1514.70 MB
Best trial: 7. Best value: 1.26376:  17%|███▍                | 13/75 [11:25<42:42, 41.33s/it, 685.63/5400 seconds]INFO:root:Model created: type=BiLSTM, layers=2, units=13, dropout=0.08643526656319755, lr=0.005378260981163437
Epoch 1/5
97/97 - 19s - 200ms/step - RootMeanSquaredError: 3.6850 - loss: 13.5792 - val_RootMeanSquaredError: 2.2554 - val_loss: 5.0869
Epoch 2/5
97/97 - 18s - 190ms/step - RootMeanSquaredError: 3.1655 - loss: 10.0202 - val_RootMeanSquaredError: 1.5946 - val_loss: 2.5427
Epoch 3/5
97/97 - 18s - 189ms/step - RootMeanSquaredError: 2.4605 - loss: 6.0542 - val_RootMeanSquaredError: 1.5198 - val_loss: 2.3097
Epoch 4/5
97/97 - 18s - 189ms/step - RootMeanSquaredError: 2.1480 - loss: 4.6141 - val_RootMeanSquaredError: 1.5167 - val_loss: 2.3002
Epoch 5/5
97/97 - 18s - 189ms/step - RootMeanSquaredError: 2.0602 - loss: 4.2443 - val_RootMeanSquaredError: 1.5657 - val_loss: 2.4514
INFO:root:Trial 13: RMSE 1.5166557006687102
[I 2025-04-30 16:14:01,674] Trial 13 finished with value: 1.5166557006687102 and parameters: {'batch_size': 32, 'model_type': 'BiLSTM', 'n_layers': 2, 'units': 13, 'dropout': 0.08643526656319755, 'lr': 0.005378260981163437}. Best is trial 7 with value: 1.2637627608092945.
Best trial: 7. Best value: 1.26376:  17%|███▍                | 13/75 [13:00<42:42, 41.33s/it, 685.63/5400 seconds]
Memory usage: 1588.31 MB
Best trial: 7. Best value: 1.26376:  19%|███▋                | 14/75 [13:00<58:28, 57.52s/it, 780.55/5400 seconds]INFO:root:Model created: type=GRU, layers=2, units=40, dropout=0.2282822224375096, lr=0.0005508596208657732
Epoch 1/5
97/97 - 6s - 65ms/step - RootMeanSquaredError: 3.7829 - loss: 14.3103 - val_RootMeanSquaredError: 2.5004 - val_loss: 6.2519
Epoch 2/5
97/97 - 6s - 59ms/step - RootMeanSquaredError: 3.6956 - loss: 13.6573 - val_RootMeanSquaredError: 2.4198 - val_loss: 5.8556
Epoch 3/5
97/97 - 6s - 59ms/step - RootMeanSquaredError: 3.5793 - loss: 12.8115 - val_RootMeanSquaredError: 2.3075 - val_loss: 5.3247
Epoch 4/5
97/97 - 6s - 59ms/step - RootMeanSquaredError: 3.4511 - loss: 11.9100 - val_RootMeanSquaredError: 2.2347 - val_loss: 4.9941
Epoch 5/5
97/97 - 6s - 59ms/step - RootMeanSquaredError: 3.3463 - loss: 11.1974 - val_RootMeanSquaredError: 2.1310 - val_loss: 4.5412
INFO:root:Trial 14: RMSE 2.1310045586475423
[I 2025-04-30 16:14:31,538] Trial 14 finished with value: 2.1310045586475423 and parameters: {'batch_size': 32, 'model_type': 'GRU', 'n_layers': 2, 'units': 40, 'dropout': 0.2282822224375096, 'lr': 0.0005508596208657732}. Best is trial 7 with value: 1.2637627608092945.
Best trial: 7. Best value: 1.26376:  19%|███▋                | 14/75 [13:30<58:28, 57.52s/it, 780.55/5400 seconds]
Memory usage: 1616.97 MB
Best trial: 7. Best value: 1.26376:  20%|████                | 15/75 [13:30<49:10, 49.18s/it, 810.41/5400 seconds]INFO:root:Model created: type=BiLSTM, layers=2, units=25, dropout=0.162593406995296, lr=0.002583339507748578
Epoch 1/5
97/97 - 19s - 197ms/step - RootMeanSquaredError: 3.7129 - loss: 13.7858 - val_RootMeanSquaredError: 2.3154 - val_loss: 5.3612
Epoch 2/5
97/97 - 18s - 189ms/step - RootMeanSquaredError: 3.3636 - loss: 11.3137 - val_RootMeanSquaredError: 1.9260 - val_loss: 3.7094
Epoch 3/5
97/97 - 18s - 190ms/step - RootMeanSquaredError: 2.9225 - loss: 8.5411 - val_RootMeanSquaredError: 1.6185 - val_loss: 2.6196
Epoch 4/5
97/97 - 18s - 190ms/step - RootMeanSquaredError: 2.5030 - loss: 6.2649 - val_RootMeanSquaredError: 1.6777 - val_loss: 2.8147
Epoch 5/5
97/97 - 18s - 189ms/step - RootMeanSquaredError: 2.2256 - loss: 4.9531 - val_RootMeanSquaredError: 1.5965 - val_loss: 2.5487
INFO:root:Trial 15: RMSE 1.5964606055412172
[I 2025-04-30 16:16:06,397] Trial 15 finished with value: 1.5964606055412172 and parameters: {'batch_size': 32, 'model_type': 'BiLSTM', 'n_layers': 2, 'units': 25, 'dropout': 0.162593406995296, 'lr': 0.002583339507748578}. Best is trial 7 with value: 1.2637627608092945.
Best trial: 7. Best value: 1.26376:  20%|████                | 15/75 [15:05<49:10, 49.18s/it, 810.41/5400 seconds]
Memory usage: 1641.16 MB
Best trial: 7. Best value: 1.26376:  21%|███▊              | 16/75 [15:05<1:01:52, 62.93s/it, 905.27/5400 seconds]INFO:root:Model created: type=GRU, layers=2, units=34, dropout=0.09042774550365965, lr=0.006390202265789342
Epoch 1/5
97/97 - 6s - 66ms/step - RootMeanSquaredError: 3.4800 - loss: 12.1103 - val_RootMeanSquaredError: 2.0884 - val_loss: 4.3613
Epoch 2/5
97/97 - 6s - 60ms/step - RootMeanSquaredError: 2.7890 - loss: 7.7783 - val_RootMeanSquaredError: 1.5049 - val_loss: 2.2646
Epoch 3/5
97/97 - 6s - 59ms/step - RootMeanSquaredError: 2.3683 - loss: 5.6088 - val_RootMeanSquaredError: 1.7279 - val_loss: 2.9855
Epoch 4/5
97/97 - 6s - 59ms/step - RootMeanSquaredError: 2.1245 - loss: 4.5136 - val_RootMeanSquaredError: 1.4242 - val_loss: 2.0282
Epoch 5/5
97/97 - 6s - 59ms/step - RootMeanSquaredError: 1.9666 - loss: 3.8675 - val_RootMeanSquaredError: 1.3219 - val_loss: 1.7474
INFO:root:Trial 16: RMSE 1.3219052833822584
[I 2025-04-30 16:16:36,417] Trial 16 finished with value: 1.3219052833822584 and parameters: {'batch_size': 32, 'model_type': 'GRU', 'n_layers': 2, 'units': 34, 'dropout': 0.09042774550365965, 'lr': 0.006390202265789342}. Best is trial 7 with value: 1.2637627608092945.
Best trial: 7. Best value: 1.26376:  21%|███▊              | 16/75 [15:35<1:01:52, 62.93s/it, 905.27/5400 seconds]
Memory usage: 1662.59 MB
Best trial: 7. Best value: 1.26376:  23%|████▌               | 17/75 [15:35<51:15, 53.03s/it, 935.29/5400 seconds]INFO:root:Model created: type=GRU, layers=1, units=49, dropout=0.00838710083304467, lr=0.0007366510768021201
Epoch 1/5
97/97 - 5s - 47ms/step - RootMeanSquaredError: 3.7527 - loss: 14.0829 - val_RootMeanSquaredError: 2.4792 - val_loss: 6.1464
Epoch 2/5
97/97 - 4s - 40ms/step - RootMeanSquaredError: 3.6801 - loss: 13.5431 - val_RootMeanSquaredError: 2.4080 - val_loss: 5.7986
Epoch 3/5
97/97 - 4s - 39ms/step - RootMeanSquaredError: 3.5994 - loss: 12.9555 - val_RootMeanSquaredError: 2.3296 - val_loss: 5.4271
Epoch 4/5
97/97 - 4s - 39ms/step - RootMeanSquaredError: 3.5195 - loss: 12.3866 - val_RootMeanSquaredError: 2.2685 - val_loss: 5.1461
Epoch 5/5
97/97 - 4s - 39ms/step - RootMeanSquaredError: 3.4488 - loss: 11.8944 - val_RootMeanSquaredError: 2.2198 - val_loss: 4.9275
INFO:root:Trial 17: RMSE 2.2197989339427973
[I 2025-04-30 16:16:56,719] Trial 17 finished with value: 2.2197989339427973 and parameters: {'batch_size': 32, 'model_type': 'GRU', 'n_layers': 1, 'units': 49, 'dropout': 0.00838710083304467, 'lr': 0.0007366510768021201}. Best is trial 7 with value: 1.2637627608092945.
Best trial: 7. Best value: 1.26376:  23%|████▌               | 17/75 [15:55<51:15, 53.03s/it, 935.29/5400 seconds]
Memory usage: 1642.92 MB
Best trial: 7. Best value: 1.26376:  24%|████▊               | 18/75 [15:55<41:02, 43.20s/it, 955.59/5400 seconds]INFO:root:Model created: type=BiLSTM, layers=2, units=9, dropout=0.2684356694459509, lr=0.005455768088040264
Epoch 1/5
194/194 - 34s - 174ms/step - RootMeanSquaredError: 3.5930 - loss: 12.9094 - val_RootMeanSquaredError: 2.1141 - val_loss: 4.4696
Epoch 2/5
194/194 - 33s - 171ms/step - RootMeanSquaredError: 2.9481 - loss: 8.6911 - val_RootMeanSquaredError: 1.6375 - val_loss: 2.6813
Epoch 3/5
194/194 - 33s - 170ms/step - RootMeanSquaredError: 2.4946 - loss: 6.2232 - val_RootMeanSquaredError: 1.7106 - val_loss: 2.9262
Epoch 4/5
194/194 - 33s - 170ms/step - RootMeanSquaredError: 2.2977 - loss: 5.2795 - val_RootMeanSquaredError: 1.3655 - val_loss: 1.8646
Epoch 5/5
194/194 - 33s - 170ms/step - RootMeanSquaredError: 2.3675 - loss: 5.6050 - val_RootMeanSquaredError: 1.4586 - val_loss: 2.1274
INFO:root:Trial 18: RMSE 1.365485543572266
[I 2025-04-30 16:19:45,010] Trial 18 finished with value: 1.365485543572266 and parameters: {'batch_size': 16, 'model_type': 'BiLSTM', 'n_layers': 2, 'units': 9, 'dropout': 0.2684356694459509, 'lr': 0.005455768088040264}. Best is trial 7 with value: 1.2637627608092945.
Best trial: 7. Best value: 1.26376:  24%|████▊               | 18/75 [18:43<41:02, 43.20s/it, 955.59/5400 seconds]
Memory usage: 1643.83 MB
Best trial: 7. Best value: 1.26376:  25%|████▎            | 19/75 [18:43<1:15:23, 80.77s/it, 1123.88/5400 seconds]INFO:root:Model created: type=LSTM, layers=2, units=19, dropout=0.051150999467327166, lr=0.0022842889830367577
Epoch 1/5
97/97 - 7s - 71ms/step - RootMeanSquaredError: 3.7670 - loss: 14.1903 - val_RootMeanSquaredError: 2.4337 - val_loss: 5.9228
Epoch 2/5
97/97 - 6s - 63ms/step - RootMeanSquaredError: 3.5374 - loss: 12.5130 - val_RootMeanSquaredError: 2.1462 - val_loss: 4.6064
Epoch 3/5
97/97 - 7s - 68ms/step - RootMeanSquaredError: 3.2257 - loss: 10.4049 - val_RootMeanSquaredError: 1.7674 - val_loss: 3.1236
Epoch 4/5
97/97 - 6s - 63ms/step - RootMeanSquaredError: 2.8422 - loss: 8.0781 - val_RootMeanSquaredError: 1.5987 - val_loss: 2.5560
Epoch 5/5
97/97 - 6s - 63ms/step - RootMeanSquaredError: 2.5761 - loss: 6.6362 - val_RootMeanSquaredError: 1.7960 - val_loss: 3.2258
INFO:root:Trial 19: RMSE 1.598739228627374
[I 2025-04-30 16:20:17,430] Trial 19 finished with value: 1.598739228627374 and parameters: {'batch_size': 32, 'model_type': 'LSTM', 'n_layers': 2, 'units': 19, 'dropout': 0.051150999467327166, 'lr': 0.0022842889830367577}. Best is trial 7 with value: 1.2637627608092945.
Best trial: 7. Best value: 1.26376:  25%|████▎            | 19/75 [19:16<1:15:23, 80.77s/it, 1123.88/5400 seconds]
Memory usage: 1664.00 MB
Best trial: 7. Best value: 1.26376:  27%|████▌            | 20/75 [19:16<1:00:43, 66.25s/it, 1156.30/5400 seconds]INFO:root:Model created: type=BiLSTM, layers=1, units=12, dropout=0.18654093549161224, lr=0.005558820694048154
Epoch 1/5
97/97 - 10s - 102ms/step - RootMeanSquaredError: 3.6334 - loss: 13.2013 - val_RootMeanSquaredError: 2.2404 - val_loss: 5.0193
Epoch 2/5
97/97 - 9s - 95ms/step - RootMeanSquaredError: 3.3126 - loss: 10.9732 - val_RootMeanSquaredError: 1.9404 - val_loss: 3.7652
Epoch 3/5
97/97 - 9s - 96ms/step - RootMeanSquaredError: 2.9886 - loss: 8.9320 - val_RootMeanSquaredError: 1.8728 - val_loss: 3.5076
Epoch 4/5
97/97 - 9s - 94ms/step - RootMeanSquaredError: 2.7674 - loss: 7.6583 - val_RootMeanSquaredError: 1.6861 - val_loss: 2.8430
Epoch 5/5
97/97 - 9s - 94ms/step - RootMeanSquaredError: 2.6772 - loss: 7.1673 - val_RootMeanSquaredError: 1.7352 - val_loss: 3.0109
INFO:root:Trial 20: RMSE 1.6861096832725948
[I 2025-04-30 16:21:05,460] Trial 20 finished with value: 1.6861096832725948 and parameters: {'batch_size': 32, 'model_type': 'BiLSTM', 'n_layers': 1, 'units': 12, 'dropout': 0.18654093549161224, 'lr': 0.005558820694048154}. Best is trial 7 with value: 1.2637627608092945.
Best trial: 7. Best value: 1.26376:  27%|████▌            | 20/75 [20:04<1:00:43, 66.25s/it, 1156.30/5400 seconds]
Memory usage: 1710.84 MB
Best trial: 7. Best value: 1.26376:  28%|█████▎             | 21/75 [20:04<54:42, 60.78s/it, 1204.34/5400 seconds]INFO:root:Model created: type=GRU, layers=2, units=31, dropout=0.10854601440027455, lr=0.008201681550650616
Epoch 1/5
97/97 - 7s - 67ms/step - RootMeanSquaredError: 3.3986 - loss: 11.5507 - val_RootMeanSquaredError: 2.1635 - val_loss: 4.6809
Epoch 2/5
97/97 - 6s - 60ms/step - RootMeanSquaredError: 2.6219 - loss: 6.8743 - val_RootMeanSquaredError: 2.0310 - val_loss: 4.1248
Epoch 3/5
97/97 - 6s - 59ms/step - RootMeanSquaredError: 2.3095 - loss: 5.3340 - val_RootMeanSquaredError: 1.7629 - val_loss: 3.1077
Epoch 4/5
97/97 - 6s - 59ms/step - RootMeanSquaredError: 2.0902 - loss: 4.3689 - val_RootMeanSquaredError: 1.3743 - val_loss: 1.8887
Epoch 5/5
97/97 - 6s - 59ms/step - RootMeanSquaredError: 2.0299 - loss: 4.1207 - val_RootMeanSquaredError: 1.3659 - val_loss: 1.8657
INFO:root:Trial 21: RMSE 1.3659179650899353
[I 2025-04-30 16:21:35,572] Trial 21 finished with value: 1.3659179650899353 and parameters: {'batch_size': 32, 'model_type': 'GRU', 'n_layers': 2, 'units': 31, 'dropout': 0.10854601440027455, 'lr': 0.008201681550650616}. Best is trial 7 with value: 1.2637627608092945.
Best trial: 7. Best value: 1.26376:  28%|█████▎             | 21/75 [20:34<54:42, 60.78s/it, 1204.34/5400 seconds]
Memory usage: 1765.77 MB
Best trial: 7. Best value: 1.26376:  29%|█████▌             | 22/75 [20:34<45:33, 51.58s/it, 1234.45/5400 seconds]INFO:root:Model created: type=GRU, layers=2, units=35, dropout=0.07488015227955158, lr=0.0065242455469165135
Epoch 1/5
97/97 - 7s - 67ms/step - RootMeanSquaredError: 3.5007 - loss: 12.2546 - val_RootMeanSquaredError: 2.1779 - val_loss: 4.7431
Epoch 2/5
97/97 - 6s - 59ms/step - RootMeanSquaredError: 2.6963 - loss: 7.2701 - val_RootMeanSquaredError: 2.0335 - val_loss: 4.1352
Epoch 3/5
97/97 - 6s - 59ms/step - RootMeanSquaredError: 2.2823 - loss: 5.2087 - val_RootMeanSquaredError: 1.7281 - val_loss: 2.9862
Epoch 4/5
97/97 - 6s - 58ms/step - RootMeanSquaredError: 2.1171 - loss: 4.4821 - val_RootMeanSquaredError: 1.3185 - val_loss: 1.7386
Epoch 5/5
97/97 - 6s - 58ms/step - RootMeanSquaredError: 2.0002 - loss: 4.0010 - val_RootMeanSquaredError: 1.3507 - val_loss: 1.8243
INFO:root:Trial 22: RMSE 1.3185430425444233
[I 2025-04-30 16:22:05,523] Trial 22 finished with value: 1.3185430425444233 and parameters: {'batch_size': 32, 'model_type': 'GRU', 'n_layers': 2, 'units': 35, 'dropout': 0.07488015227955158, 'lr': 0.0065242455469165135}. Best is trial 7 with value: 1.2637627608092945.
Best trial: 7. Best value: 1.26376:  29%|█████▌             | 22/75 [21:04<45:33, 51.58s/it, 1234.45/5400 seconds]
Memory usage: 1799.73 MB
Best trial: 7. Best value: 1.26376:  31%|█████▊             | 23/75 [21:04<39:04, 45.09s/it, 1264.40/5400 seconds]INFO:root:Model created: type=GRU, layers=2, units=25, dropout=0.06052910600347712, lr=0.004070580156542647
Epoch 1/5
97/97 - 7s - 68ms/step - RootMeanSquaredError: 3.6235 - loss: 13.1300 - val_RootMeanSquaredError: 2.1866 - val_loss: 4.7810
Epoch 2/5
97/97 - 6s - 59ms/step - RootMeanSquaredError: 3.1541 - loss: 9.9486 - val_RootMeanSquaredError: 1.7073 - val_loss: 2.9150
Epoch 3/5
97/97 - 6s - 59ms/step - RootMeanSquaredError: 2.6569 - loss: 7.0589 - val_RootMeanSquaredError: 1.6866 - val_loss: 2.8448
Epoch 4/5
97/97 - 6s - 59ms/step - RootMeanSquaredError: 2.3995 - loss: 5.7577 - val_RootMeanSquaredError: 1.6161 - val_loss: 2.6119
Epoch 5/5
97/97 - 6s - 59ms/step - RootMeanSquaredError: 2.2183 - loss: 4.9208 - val_RootMeanSquaredError: 1.6483 - val_loss: 2.7168
INFO:root:Trial 23: RMSE 1.616137106235248
[I 2025-04-30 16:22:35,685] Trial 23 finished with value: 1.616137106235248 and parameters: {'batch_size': 32, 'model_type': 'GRU', 'n_layers': 2, 'units': 25, 'dropout': 0.06052910600347712, 'lr': 0.004070580156542647}. Best is trial 7 with value: 1.2637627608092945.
Best trial: 7. Best value: 1.26376:  31%|█████▊             | 23/75 [21:34<39:04, 45.09s/it, 1264.40/5400 seconds]
Memory usage: 1828.72 MB
Best trial: 7. Best value: 1.26376:  32%|██████             | 24/75 [21:34<34:31, 40.61s/it, 1294.56/5400 seconds]INFO:root:Model created: type=GRU, layers=2, units=49, dropout=0.14069190976419474, lr=0.0065896147904106524
Epoch 1/5
97/97 - 7s - 67ms/step - RootMeanSquaredError: 3.4843 - loss: 12.1400 - val_RootMeanSquaredError: 2.0600 - val_loss: 4.2437
Epoch 2/5
97/97 - 6s - 59ms/step - RootMeanSquaredError: 2.7449 - loss: 7.5343 - val_RootMeanSquaredError: 1.7487 - val_loss: 3.0581
Epoch 3/5
97/97 - 6s - 59ms/step - RootMeanSquaredError: 2.3747 - loss: 5.6390 - val_RootMeanSquaredError: 1.9895 - val_loss: 3.9582
Epoch 4/5
97/97 - 6s - 59ms/step - RootMeanSquaredError: 2.1223 - loss: 4.5044 - val_RootMeanSquaredError: 1.3136 - val_loss: 1.7257
Epoch 5/5
97/97 - 6s - 59ms/step - RootMeanSquaredError: 2.1423 - loss: 4.5896 - val_RootMeanSquaredError: 1.3309 - val_loss: 1.7712
INFO:root:Trial 24: RMSE 1.313646838367016
[I 2025-04-30 16:23:05,773] Trial 24 finished with value: 1.313646838367016 and parameters: {'batch_size': 32, 'model_type': 'GRU', 'n_layers': 2, 'units': 49, 'dropout': 0.14069190976419474, 'lr': 0.0065896147904106524}. Best is trial 7 with value: 1.2637627608092945.
Best trial: 7. Best value: 1.26376:  32%|██████             | 24/75 [22:04<34:31, 40.61s/it, 1294.56/5400 seconds]
Memory usage: 1868.62 MB
Best trial: 7. Best value: 1.26376:  33%|██████▎            | 25/75 [22:04<31:12, 37.45s/it, 1324.65/5400 seconds]INFO:root:Model created: type=GRU, layers=2, units=51, dropout=0.15172636906216375, lr=0.002107959123290999
Epoch 1/5
97/97 - 7s - 68ms/step - RootMeanSquaredError: 3.6273 - loss: 13.1575 - val_RootMeanSquaredError: 2.2222 - val_loss: 4.9384
Epoch 2/5
97/97 - 6s - 59ms/step - RootMeanSquaredError: 3.2533 - loss: 10.5841 - val_RootMeanSquaredError: 1.7451 - val_loss: 3.0454
Epoch 3/5
97/97 - 6s - 59ms/step - RootMeanSquaredError: 2.8312 - loss: 8.0156 - val_RootMeanSquaredError: 1.9389 - val_loss: 3.7595
Epoch 4/5
97/97 - 6s - 59ms/step - RootMeanSquaredError: 2.5762 - loss: 6.6366 - val_RootMeanSquaredError: 1.6979 - val_loss: 2.8830
Epoch 5/5
97/97 - 6s - 60ms/step - RootMeanSquaredError: 2.4259 - loss: 5.8851 - val_RootMeanSquaredError: 1.7769 - val_loss: 3.1573
INFO:root:Trial 25: RMSE 1.6979434286603545
[I 2025-04-30 16:23:36,043] Trial 25 finished with value: 1.6979434286603545 and parameters: {'batch_size': 32, 'model_type': 'GRU', 'n_layers': 2, 'units': 51, 'dropout': 0.15172636906216375, 'lr': 0.002107959123290999}. Best is trial 7 with value: 1.2637627608092945.
Best trial: 7. Best value: 1.26376:  33%|██████▎            | 25/75 [22:34<31:12, 37.45s/it, 1324.65/5400 seconds]
Memory usage: 1900.33 MB
Best trial: 7. Best value: 1.26376:  35%|██████▌            | 26/75 [22:34<28:49, 35.30s/it, 1354.92/5400 seconds]INFO:root:Model created: type=GRU, layers=2, units=53, dropout=0.19896240682432603, lr=0.004055930138334355
Epoch 1/5
194/194 - 12s - 64ms/step - RootMeanSquaredError: 3.4410 - loss: 11.8407 - val_RootMeanSquaredError: 1.8187 - val_loss: 3.3078
Epoch 2/5
194/194 - 11s - 59ms/step - RootMeanSquaredError: 2.6690 - loss: 7.1236 - val_RootMeanSquaredError: 1.8032 - val_loss: 3.2517
Epoch 3/5
194/194 - 11s - 59ms/step - RootMeanSquaredError: 2.3572 - loss: 5.5566 - val_RootMeanSquaredError: 1.4793 - val_loss: 2.1885
Epoch 4/5
194/194 - 11s - 59ms/step - RootMeanSquaredError: 2.1985 - loss: 4.8332 - val_RootMeanSquaredError: 1.5591 - val_loss: 2.4307
Epoch 5/5
194/194 - 11s - 59ms/step - RootMeanSquaredError: 2.1222 - loss: 4.5036 - val_RootMeanSquaredError: 1.3413 - val_loss: 1.7992
INFO:root:Trial 26: RMSE 1.3413433632771006
[I 2025-04-30 16:24:34,756] Trial 26 finished with value: 1.3413433632771006 and parameters: {'batch_size': 16, 'model_type': 'GRU', 'n_layers': 2, 'units': 53, 'dropout': 0.19896240682432603, 'lr': 0.004055930138334355}. Best is trial 7 with value: 1.2637627608092945.
Best trial: 7. Best value: 1.26376:  35%|██████▌            | 26/75 [23:33<28:49, 35.30s/it, 1354.92/5400 seconds]
Memory usage: 1704.83 MB
Best trial: 7. Best value: 1.26376:  36%|██████▊            | 27/75 [23:33<33:51, 42.32s/it, 1413.63/5400 seconds]INFO:root:Model created: type=GRU, layers=2, units=43, dropout=0.2517833975291762, lr=0.006979725104161271
Epoch 1/5
97/97 - 7s - 68ms/step - RootMeanSquaredError: 3.4989 - loss: 12.2421 - val_RootMeanSquaredError: 1.9989 - val_loss: 3.9956
Epoch 2/5
97/97 - 6s - 59ms/step - RootMeanSquaredError: 2.8206 - loss: 7.9556 - val_RootMeanSquaredError: 2.0287 - val_loss: 4.1155
Epoch 3/5
97/97 - 6s - 59ms/step - RootMeanSquaredError: 2.3817 - loss: 5.6725 - val_RootMeanSquaredError: 1.7472 - val_loss: 3.0526
Epoch 4/5
97/97 - 6s - 59ms/step - RootMeanSquaredError: 2.2640 - loss: 5.1256 - val_RootMeanSquaredError: 1.3841 - val_loss: 1.9156
Epoch 5/5
97/97 - 6s - 58ms/step - RootMeanSquaredError: 2.1468 - loss: 4.6088 - val_RootMeanSquaredError: 1.9278 - val_loss: 3.7163
INFO:root:Trial 27: RMSE 1.3840558821155258
[I 2025-04-30 16:25:04,793] Trial 27 finished with value: 1.3840558821155258 and parameters: {'batch_size': 32, 'model_type': 'GRU', 'n_layers': 2, 'units': 43, 'dropout': 0.2517833975291762, 'lr': 0.006979725104161271}. Best is trial 7 with value: 1.2637627608092945.
Best trial: 7. Best value: 1.26376:  36%|██████▊            | 27/75 [24:03<33:51, 42.32s/it, 1413.63/5400 seconds]
Memory usage: 1673.59 MB
Best trial: 7. Best value: 1.26376:  37%|███████            | 28/75 [24:03<30:15, 38.64s/it, 1443.67/5400 seconds]INFO:root:Model created: type=LSTM, layers=2, units=36, dropout=0.3057567688282905, lr=0.0002812655398250712
Epoch 1/5
97/97 - 7s - 71ms/step - RootMeanSquaredError: 3.8139 - loss: 14.5457 - val_RootMeanSquaredError: 2.5597 - val_loss: 6.5522
Epoch 2/5
97/97 - 6s - 63ms/step - RootMeanSquaredError: 3.7995 - loss: 14.4358 - val_RootMeanSquaredError: 2.5379 - val_loss: 6.4409
Epoch 3/5
97/97 - 6s - 63ms/step - RootMeanSquaredError: 3.7708 - loss: 14.2190 - val_RootMeanSquaredError: 2.5065 - val_loss: 6.2825
Epoch 4/5
97/97 - 6s - 62ms/step - RootMeanSquaredError: 3.7319 - loss: 13.9273 - val_RootMeanSquaredError: 2.4823 - val_loss: 6.1617
Epoch 5/5
97/97 - 6s - 63ms/step - RootMeanSquaredError: 3.7061 - loss: 13.7350 - val_RootMeanSquaredError: 2.4571 - val_loss: 6.0375
INFO:root:Trial 28: RMSE 2.457125563071543
[I 2025-04-30 16:25:36,854] Trial 28 finished with value: 2.457125563071543 and parameters: {'batch_size': 32, 'model_type': 'LSTM', 'n_layers': 2, 'units': 36, 'dropout': 0.3057567688282905, 'lr': 0.0002812655398250712}. Best is trial 7 with value: 1.2637627608092945.
Best trial: 7. Best value: 1.26376:  37%|███████            | 28/75 [24:35<30:15, 38.64s/it, 1443.67/5400 seconds]
Memory usage: 1666.88 MB
Best trial: 7. Best value: 1.26376:  39%|███████▎           | 29/75 [24:35<28:06, 36.66s/it, 1475.73/5400 seconds]INFO:root:Model created: type=BiLSTM, layers=2, units=21, dropout=0.21803533537631223, lr=0.0001316998288984221
Epoch 1/5
97/97 - 21s - 215ms/step - RootMeanSquaredError: 3.8105 - loss: 14.5201 - val_RootMeanSquaredError: 2.5582 - val_loss: 6.5443
Epoch 2/5
97/97 - 20s - 205ms/step - RootMeanSquaredError: 3.7999 - loss: 14.4390 - val_RootMeanSquaredError: 2.5454 - val_loss: 6.4791
Epoch 3/5
97/97 - 20s - 205ms/step - RootMeanSquaredError: 3.7857 - loss: 14.3317 - val_RootMeanSquaredError: 2.5290 - val_loss: 6.3960
Epoch 4/5
97/97 - 20s - 204ms/step - RootMeanSquaredError: 3.7662 - loss: 14.1840 - val_RootMeanSquaredError: 2.5107 - val_loss: 6.3035
Epoch 5/5
97/97 - 20s - 204ms/step - RootMeanSquaredError: 3.7434 - loss: 14.0129 - val_RootMeanSquaredError: 2.4941 - val_loss: 6.2206
INFO:root:Trial 29: RMSE 2.4941152881950237
[I 2025-04-30 16:27:19,448] Trial 29 finished with value: 2.4941152881950237 and parameters: {'batch_size': 32, 'model_type': 'BiLSTM', 'n_layers': 2, 'units': 21, 'dropout': 0.21803533537631223, 'lr': 0.0001316998288984221}. Best is trial 7 with value: 1.2637627608092945.
Best trial: 7. Best value: 1.26376:  39%|███████▎           | 29/75 [26:18<28:06, 36.66s/it, 1475.73/5400 seconds]
Memory usage: 1623.52 MB
Best trial: 7. Best value: 1.26376:  40%|███████▌           | 30/75 [26:18<42:19, 56.44s/it, 1578.33/5400 seconds]INFO:root:Model created: type=BiLSTM, layers=2, units=29, dropout=0.14325502148452154, lr=0.0033572711240677316
Epoch 1/5
97/97 - 21s - 214ms/step - RootMeanSquaredError: 3.5991 - loss: 12.9532 - val_RootMeanSquaredError: 2.0904 - val_loss: 4.3697
Epoch 2/5
97/97 - 20s - 204ms/step - RootMeanSquaredError: 2.9430 - loss: 8.6614 - val_RootMeanSquaredError: 1.7331 - val_loss: 3.0038
Epoch 3/5
97/97 - 20s - 204ms/step - RootMeanSquaredError: 2.4037 - loss: 5.7780 - val_RootMeanSquaredError: 2.2784 - val_loss: 5.1912
Epoch 4/5
97/97 - 20s - 206ms/step - RootMeanSquaredError: 2.2056 - loss: 4.8645 - val_RootMeanSquaredError: 1.4718 - val_loss: 2.1663
Epoch 5/5
97/97 - 20s - 205ms/step - RootMeanSquaredError: 2.1086 - loss: 4.4461 - val_RootMeanSquaredError: 1.5466 - val_loss: 2.3921
INFO:root:Trial 30: RMSE 1.4718494244182043
[I 2025-04-30 16:29:02,134] Trial 30 finished with value: 1.4718494244182043 and parameters: {'batch_size': 32, 'model_type': 'BiLSTM', 'n_layers': 2, 'units': 29, 'dropout': 0.14325502148452154, 'lr': 0.0033572711240677316}. Best is trial 7 with value: 1.2637627608092945.
Best trial: 7. Best value: 1.26376:  40%|███████▌           | 30/75 [28:01<42:19, 56.44s/it, 1578.33/5400 seconds]
Memory usage: 1604.38 MB
Best trial: 7. Best value: 1.26376:  41%|███████▊           | 31/75 [28:01<51:33, 70.32s/it, 1681.01/5400 seconds]INFO:root:Model created: type=GRU, layers=2, units=35, dropout=0.08294009134033915, lr=0.00671878055283983
Epoch 1/5
97/97 - 6s - 66ms/step - RootMeanSquaredError: 3.5382 - loss: 12.5188 - val_RootMeanSquaredError: 1.8603 - val_loss: 3.4608
Epoch 2/5
97/97 - 6s - 59ms/step - RootMeanSquaredError: 2.7019 - loss: 7.3000 - val_RootMeanSquaredError: 2.1304 - val_loss: 4.5386
Epoch 3/5
97/97 - 6s - 59ms/step - RootMeanSquaredError: 2.2883 - loss: 5.2363 - val_RootMeanSquaredError: 1.6610 - val_loss: 2.7589
Epoch 4/5
97/97 - 6s - 59ms/step - RootMeanSquaredError: 2.0866 - loss: 4.3538 - val_RootMeanSquaredError: 1.4313 - val_loss: 2.0488
Epoch 5/5
97/97 - 6s - 58ms/step - RootMeanSquaredError: 2.0688 - loss: 4.2797 - val_RootMeanSquaredError: 1.4913 - val_loss: 2.2240
INFO:root:Trial 31: RMSE 1.4313467366157557
[I 2025-04-30 16:29:32,116] Trial 31 finished with value: 1.4313467366157557 and parameters: {'batch_size': 32, 'model_type': 'GRU', 'n_layers': 2, 'units': 35, 'dropout': 0.08294009134033915, 'lr': 0.00671878055283983}. Best is trial 7 with value: 1.2637627608092945.
Best trial: 7. Best value: 1.26376:  41%|███████▊           | 31/75 [28:30<51:33, 70.32s/it, 1681.01/5400 seconds]
Memory usage: 1644.45 MB
Best trial: 7. Best value: 1.26376:  43%|████████           | 32/75 [28:30<41:43, 58.22s/it, 1710.99/5400 seconds]INFO:root:Model created: type=GRU, layers=2, units=46, dropout=0.030074554803525222, lr=0.006724758150114263
Epoch 1/5
97/97 - 7s - 68ms/step - RootMeanSquaredError: 3.4759 - loss: 12.0815 - val_RootMeanSquaredError: 1.7703 - val_loss: 3.1341
Epoch 2/5
97/97 - 6s - 59ms/step - RootMeanSquaredError: 2.6419 - loss: 6.9797 - val_RootMeanSquaredError: 1.7867 - val_loss: 3.1923
Epoch 3/5
97/97 - 7s - 68ms/step - RootMeanSquaredError: 2.3391 - loss: 5.4713 - val_RootMeanSquaredError: 1.4741 - val_loss: 2.1731
Epoch 4/5
97/97 - 6s - 63ms/step - RootMeanSquaredError: 2.1546 - loss: 4.6423 - val_RootMeanSquaredError: 1.4109 - val_loss: 1.9907
Epoch 5/5
97/97 - 6s - 67ms/step - RootMeanSquaredError: 1.9489 - loss: 3.7982 - val_RootMeanSquaredError: 1.4440 - val_loss: 2.0852
INFO:root:Trial 32: RMSE 1.4109229386825843
[I 2025-04-30 16:30:04,368] Trial 32 finished with value: 1.4109229386825843 and parameters: {'batch_size': 32, 'model_type': 'GRU', 'n_layers': 2, 'units': 46, 'dropout': 0.030074554803525222, 'lr': 0.006724758150114263}. Best is trial 7 with value: 1.2637627608092945.
Best trial: 7. Best value: 1.26376:  43%|████████           | 32/75 [29:03<41:43, 58.22s/it, 1710.99/5400 seconds]
Memory usage: 1450.00 MB
Best trial: 7. Best value: 1.26376:  44%|████████▎          | 33/75 [29:03<35:17, 50.43s/it, 1743.25/5400 seconds]INFO:root:Model created: type=GRU, layers=2, units=56, dropout=0.09570506668342033, lr=0.001720970135049372
Epoch 1/5
97/97 - 7s - 74ms/step - RootMeanSquaredError: 3.6518 - loss: 13.3360 - val_RootMeanSquaredError: 2.2133 - val_loss: 4.8988
Epoch 2/5
97/97 - 6s - 67ms/step - RootMeanSquaredError: 3.2236 - loss: 10.3915 - val_RootMeanSquaredError: 1.7851 - val_loss: 3.1864
Epoch 3/5
97/97 - 7s - 70ms/step - RootMeanSquaredError: 2.7484 - loss: 7.5538 - val_RootMeanSquaredError: 1.8016 - val_loss: 3.2457
Epoch 4/5
97/97 - 7s - 70ms/step - RootMeanSquaredError: 2.5593 - loss: 6.5501 - val_RootMeanSquaredError: 1.7776 - val_loss: 3.1598
Epoch 5/5
97/97 - 7s - 70ms/step - RootMeanSquaredError: 2.4375 - loss: 5.9415 - val_RootMeanSquaredError: 1.5627 - val_loss: 2.4420
INFO:root:Trial 33: RMSE 1.562679972410941
[I 2025-04-30 16:30:39,088] Trial 33 finished with value: 1.562679972410941 and parameters: {'batch_size': 32, 'model_type': 'GRU', 'n_layers': 2, 'units': 56, 'dropout': 0.09570506668342033, 'lr': 0.001720970135049372}. Best is trial 7 with value: 1.2637627608092945.
Best trial: 7. Best value: 1.26376:  44%|████████▎          | 33/75 [29:37<35:17, 50.43s/it, 1743.25/5400 seconds]
Memory usage: 1451.30 MB
Best trial: 7. Best value: 1.26376:  45%|████████▌          | 34/75 [29:37<31:14, 45.72s/it, 1777.97/5400 seconds]INFO:root:Model created: type=GRU, layers=2, units=26, dropout=0.03263821017548737, lr=0.006148959882153611
Epoch 1/5
97/97 - 8s - 78ms/step - RootMeanSquaredError: 3.4945 - loss: 12.2114 - val_RootMeanSquaredError: 1.9862 - val_loss: 3.9450
Epoch 2/5
97/97 - 7s - 71ms/step - RootMeanSquaredError: 2.7733 - loss: 7.6911 - val_RootMeanSquaredError: 1.7444 - val_loss: 3.0429
Epoch 3/5
97/97 - 7s - 70ms/step - RootMeanSquaredError: 2.3561 - loss: 5.5511 - val_RootMeanSquaredError: 1.4674 - val_loss: 2.1534
Epoch 4/5
97/97 - 7s - 68ms/step - RootMeanSquaredError: 2.1426 - loss: 4.5909 - val_RootMeanSquaredError: 1.5949 - val_loss: 2.5436
Epoch 5/5
97/97 - 7s - 70ms/step - RootMeanSquaredError: 2.0388 - loss: 4.1565 - val_RootMeanSquaredError: 1.6366 - val_loss: 2.6783
INFO:root:Trial 34: RMSE 1.4674417026820379
[I 2025-04-30 16:31:14,455] Trial 34 finished with value: 1.4674417026820379 and parameters: {'batch_size': 32, 'model_type': 'GRU', 'n_layers': 2, 'units': 26, 'dropout': 0.03263821017548737, 'lr': 0.006148959882153611}. Best is trial 7 with value: 1.2637627608092945.
Best trial: 7. Best value: 1.26376:  45%|████████▌          | 34/75 [30:13<31:14, 45.72s/it, 1777.97/5400 seconds]
Memory usage: 1481.64 MB
Best trial: 7. Best value: 1.26376:  47%|████████▊          | 35/75 [30:13<28:24, 42.61s/it, 1813.33/5400 seconds]INFO:root:Model created: type=GRU, layers=2, units=59, dropout=0.07889360435326061, lr=0.0010518082627103624
Epoch 1/5
97/97 - 8s - 80ms/step - RootMeanSquaredError: 3.7098 - loss: 13.7627 - val_RootMeanSquaredError: 2.3452 - val_loss: 5.5000
Epoch 2/5
97/97 - 7s - 72ms/step - RootMeanSquaredError: 3.4188 - loss: 11.6885 - val_RootMeanSquaredError: 2.0930 - val_loss: 4.3807
Epoch 3/5
97/97 - 7s - 70ms/step - RootMeanSquaredError: 3.1175 - loss: 9.7190 - val_RootMeanSquaredError: 1.8099 - val_loss: 3.2759
Epoch 4/5
97/97 - 7s - 70ms/step - RootMeanSquaredError: 2.7704 - loss: 7.6752 - val_RootMeanSquaredError: 1.9226 - val_loss: 3.6963
Epoch 5/5
97/97 - 7s - 70ms/step - RootMeanSquaredError: 2.6445 - loss: 6.9931 - val_RootMeanSquaredError: 1.8654 - val_loss: 3.4796
INFO:root:Trial 35: RMSE 1.8099493928073251
[I 2025-04-30 16:31:50,327] Trial 35 finished with value: 1.8099493928073251 and parameters: {'batch_size': 32, 'model_type': 'GRU', 'n_layers': 2, 'units': 59, 'dropout': 0.07889360435326061, 'lr': 0.0010518082627103624}. Best is trial 7 with value: 1.2637627608092945.
Best trial: 7. Best value: 1.26376:  47%|████████▊          | 35/75 [30:49<28:24, 42.61s/it, 1813.33/5400 seconds]
Memory usage: 1473.75 MB
Best trial: 7. Best value: 1.26376:  48%|█████████          | 36/75 [30:49<26:22, 40.59s/it, 1849.20/5400 seconds]INFO:root:Model created: type=LSTM, layers=2, units=36, dropout=0.0010440942451677704, lr=0.00418484231926808
Epoch 1/5
97/97 - 8s - 84ms/step - RootMeanSquaredError: 3.6490 - loss: 13.3152 - val_RootMeanSquaredError: 2.2295 - val_loss: 4.9708
Epoch 2/5
97/97 - 7s - 74ms/step - RootMeanSquaredError: 3.1760 - loss: 10.0868 - val_RootMeanSquaredError: 1.7880 - val_loss: 3.1971
Epoch 3/5
97/97 - 7s - 73ms/step - RootMeanSquaredError: 2.4354 - loss: 5.9312 - val_RootMeanSquaredError: 1.7156 - val_loss: 2.9434
Epoch 4/5
97/97 - 7s - 73ms/step - RootMeanSquaredError: 2.1551 - loss: 4.6444 - val_RootMeanSquaredError: 1.8289 - val_loss: 3.3447
Epoch 5/5
97/97 - 7s - 73ms/step - RootMeanSquaredError: 2.0931 - loss: 4.3811 - val_RootMeanSquaredError: 1.4602 - val_loss: 2.1322
INFO:root:Trial 36: RMSE 1.4602088593512297
[I 2025-04-30 16:32:27,744] Trial 36 finished with value: 1.4602088593512297 and parameters: {'batch_size': 32, 'model_type': 'LSTM', 'n_layers': 2, 'units': 36, 'dropout': 0.0010440942451677704, 'lr': 0.00418484231926808}. Best is trial 7 with value: 1.2637627608092945.
Best trial: 7. Best value: 1.26376:  48%|█████████          | 36/75 [31:26<26:22, 40.59s/it, 1849.20/5400 seconds]
Memory usage: 1496.72 MB
Best trial: 7. Best value: 1.26376:  49%|█████████▎         | 37/75 [31:26<25:06, 39.64s/it, 1886.62/5400 seconds]INFO:root:Model created: type=GRU, layers=2, units=28, dropout=0.13215933838971677, lr=0.002998565618954457
Epoch 1/5
194/194 - 14s - 74ms/step - RootMeanSquaredError: 3.5287 - loss: 12.4520 - val_RootMeanSquaredError: 2.0614 - val_loss: 4.2493
Epoch 2/5
194/194 - 13s - 67ms/step - RootMeanSquaredError: 2.9032 - loss: 8.4284 - val_RootMeanSquaredError: 1.9061 - val_loss: 3.6334
Epoch 3/5
194/194 - 13s - 68ms/step - RootMeanSquaredError: 2.5908 - loss: 6.7121 - val_RootMeanSquaredError: 1.7517 - val_loss: 3.0686
Epoch 4/5
194/194 - 13s - 69ms/step - RootMeanSquaredError: 2.3578 - loss: 5.5594 - val_RootMeanSquaredError: 1.7385 - val_loss: 3.0225
Epoch 5/5
194/194 - 13s - 68ms/step - RootMeanSquaredError: 2.2766 - loss: 5.1829 - val_RootMeanSquaredError: 1.7734 - val_loss: 3.1451
INFO:root:Trial 37: RMSE 1.7385259059505613
[I 2025-04-30 16:33:35,743] Trial 37 finished with value: 1.7385259059505613 and parameters: {'batch_size': 16, 'model_type': 'GRU', 'n_layers': 2, 'units': 28, 'dropout': 0.13215933838971677, 'lr': 0.002998565618954457}. Best is trial 7 with value: 1.2637627608092945.
Best trial: 7. Best value: 1.26376:  49%|█████████▎         | 37/75 [32:34<25:06, 39.64s/it, 1886.62/5400 seconds]
Memory usage: 1497.20 MB
Best trial: 7. Best value: 1.26376:  51%|█████████▋         | 38/75 [32:34<29:41, 48.15s/it, 1954.62/5400 seconds]INFO:root:Model created: type=BiLSTM, layers=2, units=16, dropout=0.1712361413969258, lr=0.007359582585458034
Epoch 1/5
97/97 - 24s - 244ms/step - RootMeanSquaredError: 3.5894 - loss: 12.8841 - val_RootMeanSquaredError: 2.0846 - val_loss: 4.3454
Epoch 2/5
97/97 - 22s - 224ms/step - RootMeanSquaredError: 2.7476 - loss: 7.5494 - val_RootMeanSquaredError: 1.8053 - val_loss: 3.2593
Epoch 3/5
97/97 - 22s - 224ms/step - RootMeanSquaredError: 2.2892 - loss: 5.2404 - val_RootMeanSquaredError: 1.3174 - val_loss: 1.7354
Epoch 4/5
97/97 - 22s - 223ms/step - RootMeanSquaredError: 2.1430 - loss: 4.5925 - val_RootMeanSquaredError: 1.9622 - val_loss: 3.8503
Epoch 5/5
97/97 - 22s - 227ms/step - RootMeanSquaredError: 2.1868 - loss: 4.7821 - val_RootMeanSquaredError: 1.5795 - val_loss: 2.4947
INFO:root:Trial 38: RMSE 1.3173593874646832
[I 2025-04-30 16:35:29,445] Trial 38 finished with value: 1.3173593874646832 and parameters: {'batch_size': 32, 'model_type': 'BiLSTM', 'n_layers': 2, 'units': 16, 'dropout': 0.1712361413969258, 'lr': 0.007359582585458034}. Best is trial 7 with value: 1.2637627608092945.
Best trial: 7. Best value: 1.26376:  51%|█████████▋         | 38/75 [34:28<29:41, 48.15s/it, 1954.62/5400 seconds]
Memory usage: 1815.38 MB
Best trial: 7. Best value: 1.26376:  52%|█████████▉         | 39/75 [34:28<40:41, 67.81s/it, 2068.32/5400 seconds]INFO:root:Model created: type=BiLSTM, layers=2, units=14, dropout=0.1789352156599015, lr=0.007948219257599564
Epoch 1/5
97/97 - 24s - 243ms/step - RootMeanSquaredError: 3.5871 - loss: 12.8673 - val_RootMeanSquaredError: 1.9479 - val_loss: 3.7943
Epoch 2/5
97/97 - 22s - 225ms/step - RootMeanSquaredError: 2.7286 - loss: 7.4453 - val_RootMeanSquaredError: 1.6907 - val_loss: 2.8585
Epoch 3/5
97/97 - 22s - 224ms/step - RootMeanSquaredError: 2.3759 - loss: 5.6449 - val_RootMeanSquaredError: 1.4334 - val_loss: 2.0547
Epoch 4/5
97/97 - 22s - 225ms/step - RootMeanSquaredError: 2.1115 - loss: 4.4586 - val_RootMeanSquaredError: 1.3298 - val_loss: 1.7685
Epoch 5/5
97/97 - 22s - 226ms/step - RootMeanSquaredError: 2.0959 - loss: 4.3930 - val_RootMeanSquaredError: 1.5180 - val_loss: 2.3044
INFO:root:Trial 39: RMSE 1.3298412629050271
[I 2025-04-30 16:37:23,079] Trial 39 finished with value: 1.3298412629050271 and parameters: {'batch_size': 32, 'model_type': 'BiLSTM', 'n_layers': 2, 'units': 14, 'dropout': 0.1789352156599015, 'lr': 0.007948219257599564}. Best is trial 7 with value: 1.2637627608092945.
Best trial: 7. Best value: 1.26376:  52%|█████████▉         | 39/75 [36:21<40:41, 67.81s/it, 2068.32/5400 seconds]
Memory usage: 1822.58 MB
Best trial: 7. Best value: 1.26376:  53%|██████████▏        | 40/75 [36:21<47:34, 81.56s/it, 2181.96/5400 seconds]INFO:root:Model created: type=BiLSTM, layers=1, units=17, dropout=0.23687377456590936, lr=0.00971073070292231
Epoch 1/5
194/194 - 19s - 98ms/step - RootMeanSquaredError: 3.5114 - loss: 12.3300 - val_RootMeanSquaredError: 1.9548 - val_loss: 3.8211
Epoch 2/5
194/194 - 18s - 92ms/step - RootMeanSquaredError: 2.8361 - loss: 8.0437 - val_RootMeanSquaredError: 1.6548 - val_loss: 2.7383
Epoch 3/5
194/194 - 18s - 92ms/step - RootMeanSquaredError: 2.5378 - loss: 6.4404 - val_RootMeanSquaredError: 1.6019 - val_loss: 2.5659
Epoch 4/5
194/194 - 18s - 92ms/step - RootMeanSquaredError: 2.3626 - loss: 5.5817 - val_RootMeanSquaredError: 1.5894 - val_loss: 2.5263
Epoch 5/5
194/194 - 18s - 92ms/step - RootMeanSquaredError: 2.2075 - loss: 4.8732 - val_RootMeanSquaredError: 1.7600 - val_loss: 3.0977
INFO:root:Trial 40: RMSE 1.5894481205655833
[I 2025-04-30 16:38:55,272] Trial 40 finished with value: 1.5894481205655833 and parameters: {'batch_size': 16, 'model_type': 'BiLSTM', 'n_layers': 1, 'units': 17, 'dropout': 0.23687377456590936, 'lr': 0.00971073070292231}. Best is trial 7 with value: 1.2637627608092945.
Best trial: 7. Best value: 1.26376:  53%|██████████▏        | 40/75 [37:54<47:34, 81.56s/it, 2181.96/5400 seconds]
Memory usage: 1686.36 MB
Best trial: 7. Best value: 1.26376:  55%|██████████▍        | 41/75 [37:54<48:01, 84.75s/it, 2274.15/5400 seconds]INFO:root:Model created: type=BiLSTM, layers=2, units=21, dropout=0.207066296803339, lr=0.0047450711394061
Epoch 1/5
97/97 - 23s - 240ms/step - RootMeanSquaredError: 3.6153 - loss: 13.0705 - val_RootMeanSquaredError: 2.1495 - val_loss: 4.6204
Epoch 2/5
97/97 - 22s - 226ms/step - RootMeanSquaredError: 3.0087 - loss: 9.0521 - val_RootMeanSquaredError: 1.5949 - val_loss: 2.5436
Epoch 3/5
97/97 - 23s - 234ms/step - RootMeanSquaredError: 2.4401 - loss: 5.9542 - val_RootMeanSquaredError: 1.6427 - val_loss: 2.6984
Epoch 4/5
97/97 - 22s - 228ms/step - RootMeanSquaredError: 2.2877 - loss: 5.2334 - val_RootMeanSquaredError: 1.6783 - val_loss: 2.8165
INFO:root:Trial 41: RMSE 1.5948554024133812
[I 2025-04-30 16:40:28,109] Trial 41 finished with value: 1.5948554024133812 and parameters: {'batch_size': 32, 'model_type': 'BiLSTM', 'n_layers': 2, 'units': 21, 'dropout': 0.207066296803339, 'lr': 0.0047450711394061}. Best is trial 7 with value: 1.2637627608092945.
Best trial: 7. Best value: 1.26376:  55%|██████████▍        | 41/75 [39:26<48:01, 84.75s/it, 2274.15/5400 seconds]
Memory usage: 1590.92 MB
Best trial: 7. Best value: 1.26376:  56%|██████████▋        | 42/75 [39:26<47:56, 87.18s/it, 2366.99/5400 seconds]INFO:root:Model created: type=BiLSTM, layers=2, units=16, dropout=0.15976376797877256, lr=0.0075147411073303114
Epoch 1/5
97/97 - 23s - 240ms/step - RootMeanSquaredError: 3.5760 - loss: 12.7876 - val_RootMeanSquaredError: 2.0294 - val_loss: 4.1186
Epoch 2/5
97/97 - 22s - 225ms/step - RootMeanSquaredError: 2.8593 - loss: 8.1756 - val_RootMeanSquaredError: 1.7534 - val_loss: 3.0745
Epoch 3/5
97/97 - 22s - 230ms/step - RootMeanSquaredError: 2.3072 - loss: 5.3232 - val_RootMeanSquaredError: 1.5485 - val_loss: 2.3978
Epoch 4/5
97/97 - 22s - 228ms/step - RootMeanSquaredError: 2.1465 - loss: 4.6076 - val_RootMeanSquaredError: 1.4621 - val_loss: 2.1378
Epoch 5/5
97/97 - 23s - 235ms/step - RootMeanSquaredError: 2.0287 - loss: 4.1157 - val_RootMeanSquaredError: 1.3458 - val_loss: 1.8113
INFO:root:Trial 42: RMSE 1.345845576743448
[I 2025-04-30 16:42:23,387] Trial 42 finished with value: 1.345845576743448 and parameters: {'batch_size': 32, 'model_type': 'BiLSTM', 'n_layers': 2, 'units': 16, 'dropout': 0.15976376797877256, 'lr': 0.0075147411073303114}. Best is trial 7 with value: 1.2637627608092945.
Best trial: 7. Best value: 1.26376:  56%|██████████▋        | 42/75 [41:22<47:56, 87.18s/it, 2366.99/5400 seconds]
Memory usage: 1286.94 MB
Best trial: 7. Best value: 1.26376:  57%|██████████▉        | 43/75 [41:22<50:59, 95.61s/it, 2482.27/5400 seconds]INFO:root:Model created: type=BiLSTM, layers=2, units=45, dropout=0.3763341616416633, lr=0.003641121675863821
Epoch 1/5
97/97 - 24s - 244ms/step - RootMeanSquaredError: 3.5927 - loss: 12.9073 - val_RootMeanSquaredError: 2.1005 - val_loss: 4.4121
Epoch 2/5
97/97 - 22s - 227ms/step - RootMeanSquaredError: 3.0236 - loss: 9.1424 - val_RootMeanSquaredError: 1.6872 - val_loss: 2.8466
Epoch 3/5
97/97 - 22s - 227ms/step - RootMeanSquaredError: 2.5002 - loss: 6.2508 - val_RootMeanSquaredError: 2.3131 - val_loss: 5.3504
Epoch 4/5
97/97 - 22s - 228ms/step - RootMeanSquaredError: 2.5325 - loss: 6.4138 - val_RootMeanSquaredError: 1.8627 - val_loss: 3.4698
INFO:root:Trial 43: RMSE 1.6871820286050077
[I 2025-04-30 16:43:56,125] Trial 43 finished with value: 1.6871820286050077 and parameters: {'batch_size': 32, 'model_type': 'BiLSTM', 'n_layers': 2, 'units': 45, 'dropout': 0.3763341616416633, 'lr': 0.003641121675863821}. Best is trial 7 with value: 1.2637627608092945.
Best trial: 7. Best value: 1.26376:  57%|██████████▉        | 43/75 [42:54<50:59, 95.61s/it, 2482.27/5400 seconds]
Memory usage: 1394.59 MB
Best trial: 7. Best value: 1.26376:  59%|███████████▏       | 44/75 [42:55<48:57, 94.75s/it, 2575.00/5400 seconds]INFO:root:Model created: type=LSTM, layers=2, units=23, dropout=0.12112121485214368, lr=0.005310677616425162
Epoch 1/5
97/97 - 7s - 75ms/step - RootMeanSquaredError: 3.6699 - loss: 13.4685 - val_RootMeanSquaredError: 2.3105 - val_loss: 5.3384
Epoch 2/5
97/97 - 6s - 63ms/step - RootMeanSquaredError: 3.2443 - loss: 10.5256 - val_RootMeanSquaredError: 1.9144 - val_loss: 3.6649
Epoch 3/5
97/97 - 6s - 62ms/step - RootMeanSquaredError: 2.6441 - loss: 6.9913 - val_RootMeanSquaredError: 1.7601 - val_loss: 3.0981
Epoch 4/5
97/97 - 7s - 70ms/step - RootMeanSquaredError: 2.2767 - loss: 5.1834 - val_RootMeanSquaredError: 1.6601 - val_loss: 2.7560
Epoch 5/5
97/97 - 6s - 63ms/step - RootMeanSquaredError: 2.1456 - loss: 4.6037 - val_RootMeanSquaredError: 1.6526 - val_loss: 2.7311
INFO:root:Trial 44: RMSE 1.6525937290183785
[I 2025-04-30 16:44:29,239] Trial 44 finished with value: 1.6525937290183785 and parameters: {'batch_size': 32, 'model_type': 'LSTM', 'n_layers': 2, 'units': 23, 'dropout': 0.12112121485214368, 'lr': 0.005310677616425162}. Best is trial 7 with value: 1.2637627608092945.
Best trial: 7. Best value: 1.26376:  59%|███████████▏       | 44/75 [43:28<48:57, 94.75s/it, 2575.00/5400 seconds]
Memory usage: 1486.34 MB
Best trial: 7. Best value: 1.26376:  60%|███████████▍       | 45/75 [43:28<38:07, 76.26s/it, 2608.12/5400 seconds]INFO:root:Model created: type=GRU, layers=2, units=11, dropout=0.058494088833904626, lr=0.009883483236609284
Epoch 1/5
97/97 - 8s - 83ms/step - RootMeanSquaredError: 3.4710 - loss: 12.0478 - val_RootMeanSquaredError: 1.7639 - val_loss: 3.1113
Epoch 2/5



Best LGBM params: {'colsample_bytree': 1.0, 'learning_rate': 0.05, 'max_depth': 5, 'min_child_samples': 20, 'n_estimators': 200, 'num_leaves': 31, 'subsample': 0.7}
Best LGBM RMSE: 2.155572873639039

→ Diversity-based Ensemble (RNN + XGB + LGBM):
122/122 ━━━━━━━━━━━━━━━━━━━━ 15s 123ms/step
3-model Ensemble RMSE: 1.2128
Saved ensemble_3model_predictions1.csv

✅ Hyperparameter tuning complete.

**rnnmod.h5**
97/97 - 2s - 16ms/step - RootMeanSquaredError: 1.0774 - loss: 1.2810
Epoch 47/50
97/97 - 1s - 15ms/step - RootMeanSquaredError: 1.0559 - loss: 1.2354
Epoch 48/50
97/97 - 2s - 16ms/step - RootMeanSquaredError: 1.0262 - loss: 1.1736
Epoch 49/50
97/97 - 1s - 15ms/step - RootMeanSquaredError: 1.0540 - loss: 1.2312
Epoch 50/50
97/97 - 1s - 15ms/step - RootMeanSquaredError: 1.0621 - loss: 1.2486
25/25 ━━━━━━━━━━━━━━━━━━━━ 1s 27ms/step
Test RMSE: 0.8127135828899715
WARNING: absl:You are saving your model as an HDF5 file via 'model,save()' or 'keras.saving.s ave_model(model). This file format is considered legacy. We recommend using instead the nat ive Keras format, e.g. 'model.save( 'my_model, keras') or 'keras.saving save_model (model, 'my_model. keras')


RNN CV RMSE: 3.3105
XGBoost CV RMSE: 2.3225
LightGBM CV RMSE: 2.2959
Ensemble Weights: RNN=0.2586, XGBoost=0.3686, LightGBM=0.3728
Weighted Ensemble RMSE: 1.3367
'bestrnn5.h5'
'bestxgb.pkl'
'bestlgbm.pkl'

INFO:root:Initial shape: (3888, 94)
INFO:root:Manual anomalies handled: 0 price, 74 volume
INFO:root:Missing values filled: 1859
INFO:root:Removed 0 duplicate rows
INFO:root:All specified features retained after preprocessing.
INFO:root:Data Quality Score: 98.09670781893004/100
INFO:root:Preprocessing complete. Final shape: (3888, 112)
INFO:root:Final columns: ['Adj Close', 'Close', 'High', 'Low', 'Open', 'Volume', 'Volatitlity', 'daily_returns', 'percentreturn', 'price_range', 'price_change', 'MA50', 'MA200', 'volatility', 'RSI', 'EMA12', 'EMA26', 'MACD', 'Signal_Line', 'MACD_histogram', 'BB_middle', 'BB_std', 'BB_upper', 'BB_lower', 'BB_width', 'stoch_k', 'stoch_d', 'ATR', 'OBV', 'vwap', 'rolling_mean_7', 'rolling_vol_7', 'rolling_return_7', 'return_ma_7', 'return_std_7', 'rolling_mean_30', 'rolling_vol_30', 'rolling_return_30', 'return_ma_30', 'return_std_30', 'rolling_max_7', 'rolling_min_7', 'close_lag_1', 'close_lag_3', 'close_lag_7', 'close_lag_30', 'trend_strength', 'volatility_volume_ratio', 'next_day_price', 'next_day_return', 'pos_returns', 'neg_returns', 'day_of_week', 'week_of_year', 'month', 'is_weekend', 'CCI', 'Williams_%R', 'ADX', 'ADX_pos', 'ADX_neg', 'EMA9', 'EMA21', 'EMA50', 'EMA12_over_EMA26', 'EMA9_over_EMA21', 'EMA12_over_EMA50', 'EMA9_over_EMA50', 'EMA21_over_EMA50', 'EMA12_over_EMA21', 'EMA9_over_EMA26', 'EMA21_over_EMA26', 'PPO', 'KST', 'Volatility_7', 'Volatility_14', 'Volatility_21', 'Volatility_30', 'KC_Width', 'CMF', 'Force_Index', 'MFI', 'VWAP', 'Momentum_10', 'Momentum_20', 'Momentum_30', 'Momentum_60', 'DPO', 'Aroon_Osc', 'HH_LL_Ratio', 'Price_ROC', 'RSI_Divergence', 'Profit/Loss_binary', 'Profit/Loss_target_mean', 'fold', 'volatility_20', 'volatility_50', 'volatility_100', 'momentum_5', 'vol_adj_momentum_5', 'momentum_10', 'vol_adj_momentum_10', 'momentum_20', 'vol_adj_momentum_20', 'momentum_50', 'vol_adj_momentum_50', 'rsi_14', 'rsi_21', 'rsi_50', 'trend_20', 'trend_50', 'trend_100']
Features shape: (3888, 13)
Target shape: (3888,)
Feature columns: ['volatility_volume_ratio', 'stoch_k', 'stoch_d', 'rolling_vol_7', 'rolling_return_7', 'CCI', 'ADX_pos', 'price_range', 'volatility', 'Williams_%R', 'RSI', 'MACD', 'Close']
Saved All .csv Files.